import feedparser
import smtplib
from email.mime.text import MIMEText
import os
import google.generativeai as genai
from datetime import datetime
from dotenv import load_dotenv
import json

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
EMAIL_SENDER = os.getenv("EMAIL_SENDER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.getenv("EMAIL_RECEIVER")

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.0-flash')

RSS_FEEDS = {
    "Variety": "https://variety.com/feed/",
    "Deadline": "https://deadline.com/feed/"
}

def translate_and_summarize(text):
    if not GEMINI_API_KEY:
        return f"[ë²ˆì—­ ë¶ˆê°€] API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ì›ë¬¸) {text[:100]}..."
    
    try:
        prompt = f"ë‹¤ìŒ ì˜ì–´ ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³ , ê°„ëµí•˜ê²Œ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜. í˜•ì‹ì€ 'ì œëª©: [ì œëª©]', 'ìš”ì•½: [ë‚´ìš©]' ìœ¼ë¡œ í•´ì¤˜.\n\n{text}"
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨ (API Error): {e}")
        return text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜

def send_email(articles):
    if not EMAIL_SENDER or not EMAIL_PASSWORD:
        print("âš ï¸ ì´ë©”ì¼ ì„¤ì •ì´ ì—†ì–´ ë©”ì¼ì„ ë³´ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    subject = f"[StageSide] ìµœì‹  ë‰´ìŠ¤ ìš”ì•½ - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    body = "<h2>ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤</h2><br>"
    
    for article in articles:
        source = article.get('source', 'Playbill/Unknown')
        title = article.get('title', 'No Title')
        link = article.get('link', '#')
        summary = article.get('summary_kr', 'ìš”ì•½ ì—†ìŒ')
        
        body += f"<h3>{title} ({source})</h3>"
        body += f"<p><b>ì›ë¬¸ ë§í¬:</b> <a href='{link}'>{link}</a></p>"
        body += f"<p>{summary.replace('\n', '<br>')}</p><hr>"

    msg = MIMEText(body, 'html')
    msg['Subject'] = subject
    msg['From'] = EMAIL_SENDER
    msg['To'] = EMAIL_RECEIVER

    try:
        # Gmail SMTP (App Password ì‚¬ìš© ê¶Œì¥)
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(EMAIL_SENDER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! ({EMAIL_RECEIVER})")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

import time

def crawl_rss():
    articles = []
    print("ğŸ”„ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘...")
    
    for source, url in RSS_FEEDS.items():
        print(f"ğŸ“¡ {source} ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        feed = feedparser.parse(url)
        
        # ìµœì‹  3ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
        for entry in feed.entries[:3]:
            title = entry.title
            link = entry.link
            summary = entry.description if 'description' in entry else entry.title
            
            print(f"   - ë°œê²¬: {title[:30]}...")
            
            # ë²ˆì—­ ë° ìš”ì•½
            content_to_translate = f"Title: {title}\nSummary: {summary}"
            translated_text = translate_and_summarize(content_to_translate)
            
            articles.append({
                "source": source,
                "title": title,
                "link": link,
                "summary_kr": translated_text,
                "date": datetime.now().strftime("%Y-%m-%d")
            })
            time.sleep(10) # Rate Limit ë°©ì§€
            
    return articles

def save_to_json(new_data):
    file_path = 'data/articles.json'
    
    all_data = []
    # ê¸°ì¡´ ë°ì´í„° ìˆë‹¤ë©´ ë¡œë“œ
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                existing = json.load(f)
                all_data.extend(new_data)
                all_data.extend(existing) 
            except json.JSONDecodeError:
                all_data = new_data
    else:
        all_data = new_data
    
    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    seen_links = set()
    unique_data = []
    for item in all_data:
        link = item.get('link')
        if link and link not in seen_links:
            unique_data.append(item)
            seen_links.add(link)

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(unique_data[:20], f, ensure_ascii=False, indent=4) # ìµœëŒ€ 20ê°œ ìœ ì§€
    print("âœ… data/articles.json ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    crawled_data = crawl_rss()
    if crawled_data:
        save_to_json(crawled_data)
        send_email(crawled_data)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
