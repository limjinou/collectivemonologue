import feedparser
import smtplib
from email.mime.text import MIMEText
import os
import google.generativeai as genai
from datetime import datetime, timezone
from dotenv import load_dotenv
import json
import trafilatura
import concurrent.futures
import time
import requests
import bs4

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
EMAIL_SENDER = os.getenv("EMAIL_SENDER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.getenv("EMAIL_RECEIVER")
REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT", "CollectiveMonologue_Crawler/1.0")

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # Flash Latest ì‚¬ìš© (Stable version, quota friendly)
    model = genai.GenerativeModel('gemini-flash-latest')

# ë©”ì´ì € ì†ŒìŠ¤ (ë¸Œë¡œë“œì›¨ì´ / í• ë¦¬ìš°ë“œ ë©”ì´ì €)
MAJOR_FEEDS = {
    "Playbill": "https://www.playbill.com/rss",
    "BroadwayWorld": "https://www.broadwayworld.com/rss/news.xml",
    "Deadline Theater": "https://deadline.com/v/theater/feed/",
}

# ì¸ë”” ì†ŒìŠ¤ (ëŒ€í•™ë¡œ ê°ì„±, ë¹„ì˜ë¦¬, ì†Œê·œëª¨ ê·¹ì¥)
INDIE_FEEDS = {
    "American Theatre": "https://www.americantheatre.org/feed/",
    "HowlRound": "https://howlround.com/rss.xml",  # ì˜¨ë¼ì¸ ë¹„ì˜ë¦¬ ì—°ê·¹ ë§¤ê±°ì§„ HowlRound
    "TheaterMania": "https://www.theatermania.com/feed/",
}

def fetch_article_content(url):
    """Trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë³¸ë¬¸ ë° ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded)
            # ë³¸ë¬¸ HTMLì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_url = ""
            import re
            img_match = re.search(r'<img[^>]+src=["\']([^"\'>]+)["\']', downloaded)
            if img_match:
                candidate = img_match.group(1)
                # í™ˆí˜ì´ì§€ ë¡œê³  ë“± ì‘ì€ ì—ì…‹ ì´ë¯¸ì§€ ì œì™¸
                if candidate.startswith('http') and not any(x in candidate for x in ['logo', 'icon', 'avatar', 'pixel', '1x1', 'thumb']):
                    image_url = candidate
            return text, image_url
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
    return None, ""

def fetch_reddit_comments(article_url, title, keywords):
    """Reddit ìƒìœ„ ë°˜ì‘ ê²€ìƒ‰ (PRAW ëŒ€ì‹  Keyless JSON ë°©ì‹ ì‚¬ìš© + 3ë‹¨ê³„ ì•ˆì „ë§)"""
    headers = {"User-Agent": REDDIT_USER_AGENT}
    
    def search_reddit(query, time_filter="month"):
        url = f"https://www.reddit.com/search.json?q={requests.utils.quote(query)}&sort=top&t={time_filter}&limit=3"
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                children = data.get('data', {}).get('children', [])
                return [child['data'] for child in children]
            else:
                print(f"   âš ï¸ Reddit Search API Error: {resp.status_code}")
        except Exception as e:
            print(f"   âš ï¸ Reddit Search Exception: {e}")
        return []

    def get_comments(post_id):
        url = f"https://www.reddit.com/comments/{post_id}.json?sort=confidence&limit=5"
        comments = []
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                if len(data) > 1:
                    comment_children = data[1].get('data', {}).get('children', [])
                    for child in comment_children:
                        if child['kind'] == 't1':  # t1 ì€ ëŒ“ê¸€ì„ ì˜ë¯¸
                            body = child['data'].get('body', '')
                            if len(body) > 10 and "[deleted]" not in body:
                                comments.append(body.replace('\n', ' '))
        except Exception as e:
            pass
        return comments

    try:
        submissions = []
        # 1ë‹¨ê³„: URL ê¸°ë°˜ ì•„ì£¼ ì •í™•í•œ ê²€ìƒ‰
        search_query = f'url:"{article_url}"'
        submissions = search_reddit(search_query, time_filter="month")
        time.sleep(1) # Rate limit ë°©ì§€
        
        # 2ë‹¨ê³„: URL ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ, ì œëª© ê¸°ë°˜ ì¶”ì • ê²€ìƒ‰
        if not submissions:
            import re
            clean_title = re.sub(r'[^\w\s]', '', title).strip()
            short_title = " ".join(clean_title.split()[:5])
            
            search_query = f'"{short_title}"'
            submissions = search_reddit(search_query, time_filter="week")
            time.sleep(1)
            
            # 3ë‹¨ê³„: LLM êµì°¨ ê²€ì¦
            if submissions:
                candidate = submissions[0]
                validation_prompt = f"""
                ë‰´ìŠ¤ ê¸°ì‚¬ ì›ë³¸ ì œëª©: "{title}"
                ë ˆë”§ ê²Œì‹œë¬¼ ì œëª©: "{candidate.get('title', '')}"
                ë‘ ì œëª©ì´ ì •í™•í•˜ê²Œ ë™ì¼í•œ ë‰´ìŠ¤ ë‚´ìš©ì— ëŒ€í•´ ë°˜ì‘í•˜ê³  ìˆìŠµë‹ˆê¹Œ?
                ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ê±´ì´ ì•„ë‹ˆë¼, ì •í™•íˆ ê°™ì€ ì‚¬ê±´ì¸ê°€ìš”?
                ì˜¤ì§ "True" ë˜ëŠ” "False" ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
                """
                try:
                    val_res = model.generate_content(validation_prompt).text.strip().lower()
                    if "true" not in val_res:
                        print(f"   âš ï¸ Reddit í† í”½ ë¶ˆì¼ì¹˜ë¡œ ë°°ì œ: {candidate.get('title', '')[:30]}")
                        return ""
                except Exception as eval_e:
                    print(f"   âš ï¸ Reddit ê²€ì¦ ë‹¨ê³„ ë¬´ì‹œ (ì—ëŸ¬): {eval_e}")
                    return ""
                
        # ìµœì¢… ìˆ˜ì§‘ ë¡œì§
        if submissions:
            best_post = submissions[0]
            post_id = best_post.get('id')
            subreddit_name = best_post.get('subreddit')
            
            top_comments = get_comments(post_id)
            time.sleep(1)
                    
            if top_comments:
                print(f"   ğŸ’¬ Reddit ë°˜ì‘ í™•ë³´ (r/{subreddit_name}): ëŒ“ê¸€ {len(top_comments)}ê°œ")
                return "\n".join([f"- {c}" for c in top_comments])
                
    except Exception as e:
        print(f"   âš ï¸ Reddit íŒŒì‹± ì‹¤íŒ¨: {e}")
        
    return ""

def fetch_wikipedia_image(keywords):
    """AI ì¶”ì¶œ í‚¤ì›Œë“œë¥¼ Wikipedia APIë¡œ ê²€ìƒ‰í•˜ì—¬ ì´ë¯¸ì§€ URL ë°˜í™˜ (CC ë¼ì´ì„ ìŠ¤)"""
    for keyword in keywords[:4]:  # ìµœëŒ€ 4ê°œ í‚¤ì›Œë“œ ìˆœì„œëŒ€ë¡œ ì‹œë„
        try:
            url = (
                "https://en.wikipedia.org/w/api.php"
                f"?action=query&titles={requests.utils.quote(str(keyword))}"
                "&prop=pageimages&format=json&pithumbsize=800"
            )
            resp = requests.get(url, timeout=5,
                                headers={"User-Agent": "CollectiveMonologue/1.0"})
            data = resp.json()
            pages = data.get("query", {}).get("pages", {})
            for page in pages.values():
                thumb = page.get("thumbnail", {}).get("source", "")
                if thumb:
                    print(f"   ğŸ–¼ï¸ Wikipedia ì´ë¯¸ì§€ í™•ë³´: [{keyword}]")
                    return thumb
        except Exception as e:
            print(f"   âš ï¸ Wikipedia ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
    return ""

def translate_and_summarize(text, title, reddit_comments=""):
    if not GEMINI_API_KEY:
        return {"title_en": title, "summary_en": "No API Key provided.", "keywords": []}

    if not text or len(text) < 50:
        return {"title_en": title, "summary_en": "Content too short or extraction failed.", "keywords": []}
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë³´ëƒ„ (í† í° ì œí•œ ë°©ì§€)
    truncated_text = text[:4000]

    reddit_section = ""
    if reddit_comments:
        reddit_section = f"""
    Additional Context (Reddit Comments - Local Fan Reactions):
    {reddit_comments}
    
    IMPORTANT: You have local fan reactions from Reddit. Synthesize these authentic reactions into your editorial. Describe what the US fans are excited about, worried about, or debating regarding this news. This is crucial for adding cultural depth.
        """

    prompt = f"""
    You are the editor of "Collective Monologue", a Korean-language magazine dedicated to covering American theater and film with depth, nuance, and cultural context.
    
    Below is an article titled '{title}'. Your task is NOT a simple translation.
    Instead, produce a rich, original Korean editorial that:

    1. Summarizes the core news from the article
    2. Adds meaningful background knowledge YOU ALREADY KNOW about:
       - Any ACTORS or DIRECTORS mentioned: their notable past works, career highlights, and what makes them significant
       - Any PRODUCTIONS or PLAYS mentioned: the original playwright, a brief synopsis, the work's historical/cultural significance
       - Any THEATERS or VENUES mentioned: their location, founding history, notable past productions, or their role in American theater
       - Any AWARDS or EVENTS mentioned: the history and significance of the award or event
    3. Includes a brief editorial perspective or "editor's note" that helps Korean readers understand WHY this news matters in the context of American theater/film culture
    {reddit_section}

    Write as a knowledgeable Korean cultural journalist â€” warm, insightful, and informative.
    The output must be a JSON object with KOREAN text for title_kr, summary_kr, and content_kr:
    {{
        "title_kr": "í•œêµ­ ë…ìì˜ í¥ë¯¸ë¥¼ ëŒ ìˆ˜ ìˆëŠ” ë§¤ë ¥ì ì¸ ê¸°ì‚¬ ì œëª© (í•œêµ­ì–´)",
        "summary_kr": "ë©”ì¸ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ì— í‘œì‹œë  1-2ë¬¸ì¥ì˜ í•µì‹¬ ìš”ì•½. ë…ìê°€ í´ë¦­í•˜ê³  ì‹¶ê²Œ ë§Œë“¤ì–´ë¼ (í•œêµ­ì–´)",
        "content_kr": "ê¸°ì‚¬ ë³¸ë¬¸. ë‰´ìŠ¤ ìš”ì•½ + ë“±ì¥ ì¸ë¬¼/ì‘í’ˆ/ê³µì—°ì¥ì— ëŒ€í•œ ë°°ê²½ ì§€ì‹ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì¸ í’ë¶€í•œ í…ìŠ¤íŠ¸. ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì–´ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„±. (ë§Œì•½ Reddit ëŒ“ê¸€ ì„¹ì…˜ì´ ì£¼ì–´ì¡Œë‹¤ë©´ ë³¸ë¬¸ ë‚´ì— 'í•´ì™¸ ë§¤ë‹ˆì•„ ë°˜ì‘' íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì„œ ë°˜ì˜í•  ê²ƒ) ë§ˆì§€ë§‰ì—” 'í¸ì§‘ì ì£¼' í•œ ë¬¸ë‹¨ì„ ì¶”ê°€í•  ê²ƒ (í•œêµ­ì–´)",
        "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
    }}

    Article Body:
    {truncated_text}
    """

    # ì¬ì‹œë„ ë¡œì§ (Exponential Backoff)
    max_retries = 3
    base_delay = 5  # 5ì´ˆë¶€í„° ì‹œì‘

    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
            return json.loads(response.text)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                wait_time = base_delay * (2 ** attempt) + (attempt * 2) # Jitter ì¶”ê°€
                print(f"âš ï¸ Quota exceeded. Retrying in {wait_time}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ Summary failed (API Error): {e}")
                break
    
    return {
        "title_en": title,
        "summary_en": "Summarization failed.",
        "title_kr": title,
        "summary_kr": "ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "content_kr": "ë³¸ë¬¸ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
        "keywords": []
    }

def fetch_broadway_grosses():
    """Playbill.comì—ì„œ ì´ë²ˆ ì£¼ ë¸Œë¡œë“œì›¨ì´ ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = "https://www.playbill.com/grosses"
    headers = {"User-Agent": REDDIT_USER_AGENT}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = bs4.BeautifulSoup(resp.content, 'html.parser')
            table = soup.find('table')
            if not table:
                return []
            
            rows = table.find('tbody').find_all('tr') if table.find('tbody') else table.find_all('tr')[1:]
            grosses = []
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 8:
                    # ì œëª© íƒœê·¸ ì•ˆì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ê·¹ì¥ëª… ë¶„ë¦¬ ìœ„í•¨)
                    show_node = cols[0].find('a')
                    show_name = show_node.get_text(strip=True) if show_node else cols[0].get_text(strip=True).split('Theatre')[0]
                    
                    gross_str = cols[1].get_text(strip=True)
                    capacity_str = cols[6].get_text(strip=True)
                    
                    try:
                        parsed_gross = float(gross_str.replace('$', '').replace(',', ''))
                        grosses.append({
                            "show": show_name,
                            "gross_formatted": gross_str,
                            "gross": parsed_gross,
                            "capacity": capacity_str
                        })
                    except ValueError:
                        continue
                        
            # ë§¤ì¶œì•¡(Gross) ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ ì¶”ì¶œ
            grosses.sort(key=lambda x: x['gross'], reverse=True)
            for i, item in enumerate(grosses[:5]):
                item['rank'] = i + 1
            return grosses[:5]
    except Exception as e:
        print(f"   âš ï¸ ë¸Œë¡œë“œì›¨ì´ ë°•ìŠ¤ì˜¤í”¼ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
    return []

def generate_weekly_recommendations(articles_data):
    """ìµœê·¼ ì¸ë”” ë§¤ì²´ ì¤‘ì‹¬ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´/ì‹œì¹´ê³  ì¶”ì²œì‘ 3ê°œë¥¼ ë½‘ìŠµë‹ˆë‹¤."""
    if not GEMINI_API_KEY or not articles_data:
        return []

    context = ""
    for idx, article in enumerate(articles_data[:10]):
        context += f"[{idx+1}] ì œëª©: {article['title_kr']}\nìš”ì•½: {article['summary_kr']}\n\n"
        
    prompt = f"""
    ë‹¹ì‹ ì€ ë¯¸êµ­ ì—°ê·¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì´ë²ˆ ì£¼ ìˆ˜ì§‘ëœ ì—°ê·¹ ê¸°ì‚¬ë“¤ì˜ ëª©ë¡ì…ë‹ˆë‹¤.
    ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´ ë˜ëŠ” ì‹œì¹´ê³  ë“± ì§€ì—­ ì—°ê·¹/í™”ì œì‘ ì¤‘ ì¶”ì²œí•  ë§Œí•œ ì‘í’ˆ 3ê°œ**ë¥¼ ì„ ì •í•´ ì§§ê²Œ ì¶”ì²œ ì´ìœ ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ê¸°ì‚¬ ëª©ë¡:
    {context}

    ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”. ê° ê°ì²´ëŠ” "title" (ì‘í’ˆëª…, í•œê¸€/ì˜ë¬¸ ë³‘ê¸°), "reason" (ì¶”ì²œ ì´ìœ , 2ë¬¸ì¥ ì´ë‚´) í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
    ê¸°ì‚¬ ë‚´ìš© ì¤‘ ì¶”ì²œí•  ë§Œí•œ êµ¬ì²´ì ì¸ ì—°ê·¹ ì‘í’ˆì´ ë¶€ì¡±í•˜ë‹¤ë©´, í˜„ì¬ ë¯¸êµ­ì—ì„œ í‰ë‹¨ì˜ ë†’ì€ ì§€ì§€ë¥¼ ë°›ê³  ìˆëŠ” ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´ í™”ì œì‘ì„ ì„ì˜ë¡œ ê³¨ë¼ë„ ì¢‹ìŠµë‹ˆë‹¤.
    
    [
        {{"title": "ë¯¼ì¤‘ì˜ ì  (An Enemy of the People)", "reason": "ì œë ˆë¯¸ ìŠ¤íŠ¸ë¡±ì˜ ëª…ì—°ê¸°ì™€ í•¨ê»˜ í™˜ê²½ ë¬¸ì œë¼ëŠ” ì‹œëŒ€ì  í™”ë‘ë¥¼ ë˜ì§€ëŠ” í•„ëŒ ì—°ê·¹ì…ë‹ˆë‹¤."}}
    ]
    ë‹µë³€ì€ ì˜¤ì§ JSON í˜•ì‹ìœ¼ë¡œë§Œ í•´ì£¼ì„¸ìš”.
    """
    try:
        response = model.generate_content(prompt).text.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.endswith("```"):
            response = response[:-3]
        return json.loads(response.strip())
    except Exception as e:
        print(f"   âš ï¸ ì£¼ê°„ ì¶”ì²œì‘ ìƒì„± ì‹¤íŒ¨: {e}")
    return []

def process_entry(entry, source, tier):
    """Process individual article (for parallel execution)"""
    title = entry.title
    link = entry.link
    published = entry.get('published', datetime.now().strftime("%Y-%m-%d"))
    
    print(f"   Analyzing [{tier.upper()}]: {title[:30]}...")

    # 1. Extract full text + image from article HTML
    full_text, html_image = fetch_article_content(link)
    
    # 2. Extract local reactions from Reddit (URL & Title based)
    reddit_comments = fetch_reddit_comments(link, title, [])
    
    # 3. AI Summary with Reddit context
    ai_result = translate_and_summarize(full_text, title, reddit_comments)

    # Extract image: RSS metadata ìš°ì„ , ì—†ìœ¼ë©´ HTML íŒŒì‹±, ê·¸ë˜ë„ ì—†ìœ¼ë©´ Wikipedia ê²€ìƒ‰
    image_url = html_image  # ê¸°ë³¸ê°’: HTMLì—ì„œ ì¶”ì¶œí•œ ì´ë¯¸ì§€
    if 'media_content' in entry and len(entry.media_content) > 0:
        image_url = entry.media_content[0].get('url', '') or html_image
    elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
        image_url = entry.media_thumbnail[0].get('url', '') or html_image
    elif 'links' in entry:
        for link_item in entry.links:
            if link_item.get('type', '').startswith('image/'):
                image_url = link_item.get('href', '') or html_image
                break

    # RSSë‚˜ HTMLì—ì„œ ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ Wikipedia ì´ë¯¸ì§€ ê²€ìƒ‰
    if not image_url and ai_result.get('keywords'):
        # AIê°€ ì¶”ì¶œí•œ ì˜ë¬¸ í‚¤ì›Œë“œë¡œ ì§ì ‘ ê²€ìƒ‰ (ì›ë¬¸ ì œëª©ì—ì„œ ë¡œë§ˆì ì°¾ê¸°)
        import re
        # ì˜ë¬¸ ë‹¨ì–´ê°€ í¬í•¨ëœ í‚¤ì›Œë“œ ìš°ì„  (ex. ë°°ìš° ì´ë¦„ ë“±)
        original_title_words = entry.title
        en_keywords = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+', original_title_words)
        search_keywords = en_keywords + ai_result.get('keywords', [])
        image_url = fetch_wikipedia_image(search_keywords)

    return {
        "source": source,
        "tier": tier,  # 'major' ë˜ëŠ” 'indie'
        "original_title": title,
        "link": link,
        "image": image_url,
        "title_kr": ai_result.get('title_kr', title),
        "summary_kr": ai_result.get('summary_kr', 'ë‚´ìš© ì—†ìŒ'),
        "content_kr": ai_result.get('content_kr', 'ë‚´ìš© ì—†ìŒ'),
        "keywords": ai_result.get('keywords', []),
        "date": published,
        "scraped_at": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S") # Changed to timezone.utc
    }

def send_email(articles):
    if not EMAIL_SENDER or not EMAIL_PASSWORD or not articles:
        return

    subject = f"[StageSide] Latest News Briefing - {datetime.now().strftime('%Y-%m-%d %H:%M')}" # Kept datetime.now() as per instruction for subject
    body = "<h2>Today's Top News</h2><br>"
    
    for article in articles:
        body += f"<h3>[{article['source']}] {article['title_kr']}</h3>"
        body += f"<p>{article['summary_kr']}</p>"
        body += f"<p><small>Keywords: {', '.join(article['keywords'])}</small></p>"
        body += f"<p><a href='{article['link']}' target='_blank'>Read Original</a></p><hr>"

    msg = MIMEText(body, 'html')
    msg['Subject'] = subject
    msg['From'] = EMAIL_SENDER
    msg['To'] = EMAIL_RECEIVER

    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(EMAIL_SENDER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë ˆí¬íŠ¸ ë°œì†¡ ì™„ë£Œ ({len(articles)}ê±´)")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def save_to_json(major_articles, indie_articles):
    file_path = 'data/articles.json'
    
    # ë©”ì´ì € 2ê°œ + ì¸ë”” 2ê°œ ìœ ì§€
    final_data = major_articles[:2] + indie_articles[:2]

    os.makedirs("data", exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: ë©”ì´ì € {len(major_articles[:2])}ê±´ + ì¸ë”” {len(indie_articles[:2])}ê±´ = ì´ {len(final_data)}ê±´")

def crawl_rss():
    print("ğŸš€ í¬ë¡¤ëŸ¬(ver.2) ì‹œì‘ â€” ë©”ì´ì € 2ê±´ + ì¸ë”” 2ê±´ ìˆ˜ì§‘")
    
    def fetch_from_feeds(feeds_dict, tier):
        entries = []
        for source, url in feeds_dict.items():
            print(f"ğŸ“¡ [{tier.upper()}] {source} ê²€ìƒ‰ ì¤‘...")
            try:
                feed = feedparser.parse(url)
                # ê° ì†ŒìŠ¤ë³„ ìµœì‹  2ê°œì”© ìˆ˜ì§‘ (ë²„í¼ í™•ë³´: 1ê°œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ê²ƒìœ¼ë¡œ ëŒ€ì²´)
                for entry in feed.entries[:2]:
                    entries.append((entry, source, tier))
            except Exception as e:
                print(f"âš ï¸ {source} í”¼ë“œ ì˜¤ë¥˜: {e}")
        return entries

    major_entries = fetch_from_feeds(MAJOR_FEEDS, 'major')
    indie_entries = fetch_from_feeds(INDIE_FEEDS, 'indie')
    all_entries = major_entries + indie_entries
    
    print(f"ì´ {len(all_entries)}ê°œ ê¸°ì‚¬ ë°œê²¬. ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

    major_results = []
    indie_results = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_entry = {
            executor.submit(process_entry, entry, source, tier): (entry, source, tier)
            for entry, source, tier in all_entries
        }
        for future in concurrent.futures.as_completed(future_to_entry):
            try:
                data = future.result()
                if data['tier'] == 'major':
                    major_results.append(data)
                else:
                    indie_results.append(data)
            except Exception as exc:
                print(f"âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {exc}")

    return major_results, indie_results

if __name__ == "__main__":
    major_data, indie_data = crawl_rss()
    if major_data or indie_data:
        save_to_json(major_data, indie_data)
        all_data = major_data + indie_data
        send_email(all_data)
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
