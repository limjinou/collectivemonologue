import feedparser
import smtplib
from email.mime.text import MIMEText
import os
import google.generativeai as genai
from datetime import datetime, timezone
from dotenv import load_dotenv
import json
import trafilatura
import concurrent.futures
import time
import requests
import bs4

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
EMAIL_SENDER = os.getenv("EMAIL_SENDER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.getenv("EMAIL_RECEIVER")
REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT", "CollectiveMonologue_Crawler/1.0")

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # Flash Latest ì‚¬ìš© (Stable version, quota friendly)
    model = genai.GenerativeModel('gemini-flash-latest')

# ë©”ì´ì € ì†ŒìŠ¤ (ë¸Œë¡œë“œì›¨ì´ / í• ë¦¬ìš°ë“œ ë©”ì´ì €)
MAJOR_FEEDS = {
    "Playbill": "https://www.playbill.com/rss",
    "BroadwayWorld": "https://www.broadwayworld.com/rss/news.xml",
    "Deadline Theater": "https://deadline.com/v/theater/feed/",
    "The Hollywood Reporter": "https://www.hollywoodreporter.com/feed/",
    "IndieWire": "https://www.indiewire.com/feed/",
    "Variety Theater": "https://variety.com/v/legit/feed/",
}

# ì¸ë”” ì†ŒìŠ¤ (ëŒ€í•™ë¡œ ê°ì„±, ë¹„ì˜ë¦¬, ì†Œê·œëª¨ ê·¹ì¥)
INDIE_FEEDS = {
    "American Theatre": "https://www.americantheatre.org/feed/",
    "HowlRound": "https://howlround.com/rss.xml",  # ì˜¨ë¼ì¸ ë¹„ì˜ë¦¬ ì—°ê·¹ ë§¤ê±°ì§„ HowlRound
    "TheaterMania": "https://www.theatermania.com/feed/",
    "Backstage": "https://www.backstage.com/magazine/article/feed/",
}

def fetch_article_content(url):
    """Trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë³¸ë¬¸ ë° ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded)
            # ë³¸ë¬¸ HTMLì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_url = ""
            import re
            img_match = re.search(r'<img[^>]+src=["\']([^"\'>]+)["\']', downloaded)
            if img_match:
                candidate = img_match.group(1)
                # í™ˆí˜ì´ì§€ ë¡œê³  ë“± ì‘ì€ ì—ì…‹ ì´ë¯¸ì§€ ì œì™¸
                if candidate.startswith('http') and not any(x in candidate for x in ['logo', 'icon', 'avatar', 'pixel', '1x1', 'thumb']):
                    image_url = candidate
            return text, image_url
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
    return None, ""

def fetch_reddit_comments(article_url, title, keywords):
    """Reddit ì„œë¸Œë ˆë”§ Hot ê²Œì‹œë¬¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë°˜ì‘ ìˆ˜ì§‘ (í‚¤ì›Œë“œ ë§¤ì¹­ ë°©ì‹)"""
    headers = {"User-Agent": REDDIT_USER_AGENT}

    def get_comments(post_id):
        url = f"https://www.reddit.com/comments/{post_id}.json?sort=confidence&limit=5"
        comments = []
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                if len(data) > 1:
                    comment_children = data[1].get('data', {}).get('children', [])
                    for child in comment_children:
                        if child['kind'] == 't1':
                            body = child['data'].get('body', '')
                            if len(body) > 10 and "[deleted]" not in body:
                                comments.append(body.replace('\n', ' '))
        except Exception:
            pass
        return comments

    def fetch_subreddit_hot(subreddit, limit=15):
        """ì„œë¸Œë ˆë”§ì˜ ìµœì‹ (Hot) ê²Œì‹œë¬¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                children = resp.json().get('data', {}).get('children', [])
                return [c['data'] for c in children]
        except Exception:
            pass
        return []

    import re
    try:
        # ê¸°ì‚¬ ì œëª©ì˜ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì§§ì€ ë‹¨ì–´ ë° ë¶ˆìš©ì–´ ì œê±°)
        stopwords = {'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'is', 'are', 'was', 'will', 'with', 'that', 'this', 'from', 'by', 'as', 'it', 'new', 'its', 'into', 'has', 'have', 'set'}
        title_words = set(w.lower() for w in re.findall(r'\b[A-Za-z]{4,}\b', title) if w.lower() not in stopwords)

        subreddits_to_try = ['Broadway', 'theater', 'movies', 'boxoffice']
        best_post = None
        best_score = 0

        for subreddit in subreddits_to_try:
            posts = fetch_subreddit_hot(subreddit, limit=15)
            time.sleep(0.5)
            for post in posts:
                post_title = post.get('title', '')
                post_words = set(w.lower() for w in re.findall(r'\b[A-Za-z]{4,}\b', post_title))
                overlap = len(title_words & post_words)
                if overlap > best_score:
                    best_score = overlap
                    best_post = post
                    best_post['_subreddit'] = subreddit

        # ê³µí†µ í‚¤ì›Œë“œ 1ê°œ ì´ìƒì´ë©´ ê´€ë ¨ ê²Œì‹œë¬¼ë¡œ ì¸ì • (ì—„ê²© ê¸°ì¤€ë³´ë‹¤ ìˆ˜ì§‘ë¥  ìš°ì„ )
        if best_post and best_score >= 1:
            post_id = best_post.get('id')
            subreddit_name = best_post.get('_subreddit', best_post.get('subreddit', ''))
            top_comments = get_comments(post_id)
            time.sleep(0.5)
            if top_comments:
                print(f"   ğŸ’¬ Reddit ë°˜ì‘ í™•ë³´ (r/{subreddit_name}, overlap={best_score}): ëŒ“ê¸€ {len(top_comments)}ê°œ")
                return "\n".join([f"- {c}" for c in top_comments])
            else:
                print(f"   âš ï¸ Reddit ê²Œì‹œë¬¼ ë°œê²¬í–ˆì§€ë§Œ ëŒ“ê¸€ ì—†ìŒ")
        else:
            print(f"   â„¹ï¸ Reddit ê´€ë ¨ ê²Œì‹œë¬¼ ì—†ìŒ (best overlap={best_score})")

    except Exception as e:
        print(f"   âš ï¸ Reddit íŒŒì‹± ì‹¤íŒ¨: {e}")

    return ""

def fetch_wikipedia_image(keywords):
    """AI ì¶”ì¶œ í‚¤ì›Œë“œë¥¼ Wikipedia APIë¡œ ê²€ìƒ‰í•˜ì—¬ ì´ë¯¸ì§€ URL ë°˜í™˜ (CC ë¼ì´ì„ ìŠ¤)"""
    for keyword in keywords[:4]:  # ìµœëŒ€ 4ê°œ í‚¤ì›Œë“œ ìˆœì„œëŒ€ë¡œ ì‹œë„
        try:
            url = (
                "https://en.wikipedia.org/w/api.php"
                f"?action=query&titles={requests.utils.quote(str(keyword))}"
                "&prop=pageimages&format=json&pithumbsize=800"
            )
            resp = requests.get(url, timeout=5,
                                headers={"User-Agent": "CollectiveMonologue/1.0"})
            data = resp.json()
            pages = data.get("query", {}).get("pages", {})
            for page in pages.values():
                thumb = page.get("thumbnail", {}).get("source", "")
                if thumb:
                    print(f"   ğŸ–¼ï¸ Wikipedia ì´ë¯¸ì§€ í™•ë³´: [{keyword}]")
                    return thumb
        except Exception as e:
            print(f"   âš ï¸ Wikipedia ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
    return ""

def translate_and_summarize(text, title, reddit_comments=""):
    if not GEMINI_API_KEY:
        return {"title_en": title, "summary_en": "No API Key provided.", "keywords": []}

    if not text or len(text) < 50:
        return {"title_en": title, "summary_en": "Content too short or extraction failed.", "keywords": []}
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë³´ëƒ„ (í† í° ì œí•œ ë°©ì§€)
    truncated_text = text[:4000]

    reddit_section = ""
    if reddit_comments:
        reddit_section = f"""
    Additional Context (Reddit Comments - Local Fan Reactions):
    {reddit_comments}
    
    IMPORTANT: You have local fan reactions from Reddit. Synthesize these authentic reactions into your editorial. Describe what the US fans are excited about, worried about, or debating regarding this news. This is crucial for adding cultural depth.
        """

    prompt = f"""
    You are the editor of "Collective Monologue", a Korean-language magazine dedicated to covering American theater and film with depth, nuance, and cultural context.
    
    Below is an article titled '{title}'. Your task is NOT a simple translation.
    Instead, produce a rich, original Korean editorial that:

    1. Summarizes the core news from the article
    2. Adds meaningful background knowledge YOU ALREADY KNOW about:
       - Any ACTORS or DIRECTORS mentioned: their notable past works, career highlights, and what makes them significant
       - Any PRODUCTIONS or PLAYS mentioned: the original playwright, a brief synopsis, the work's historical/cultural significance
       - Any THEATERS or VENUES mentioned: their location, founding history, notable past productions, or their role in American theater
       - Any AWARDS or EVENTS mentioned: the history and significance of the award or event
    3. Includes a brief editorial perspective or "editor's note" that helps Korean readers understand WHY this news matters in the context of American theater/film culture
    {reddit_section}

    Write as a knowledgeable Korean cultural journalist â€” warm, insightful, and informative.
    The output must be a JSON object with KOREAN text for title_kr, summary_kr, and content_kr:
    {{
        "title_kr": "í•œêµ­ ë…ìì˜ í¥ë¯¸ë¥¼ ëŒ ìˆ˜ ìˆëŠ” ë§¤ë ¥ì ì¸ ê¸°ì‚¬ ì œëª© (í•œêµ­ì–´)",
        "summary_kr": "ë©”ì¸ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ì— í‘œì‹œë  1-2ë¬¸ì¥ì˜ í•µì‹¬ ìš”ì•½. ë…ìê°€ í´ë¦­í•˜ê³  ì‹¶ê²Œ ë§Œë“¤ì–´ë¼ (í•œêµ­ì–´)",
        "content_kr": "ê¸°ì‚¬ ë³¸ë¬¸. ë‰´ìŠ¤ ìš”ì•½ + ë“±ì¥ ì¸ë¬¼/ì‘í’ˆ/ê³µì—°ì¥ì— ëŒ€í•œ ë°°ê²½ ì§€ì‹ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì¸ í’ë¶€í•œ í…ìŠ¤íŠ¸. ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì–´ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„±. ë§ˆì§€ë§‰ì—” 'í¸ì§‘ì ì£¼' í•œ ë¬¸ë‹¨ì„ ì¶”ê°€í•  ê²ƒ (í•œêµ­ì–´)",
        "reddit_reaction_kr": "ë§Œì•½ Reddit ëŒ“ê¸€ì´ ì£¼ì–´ì¡Œë‹¤ë©´, í˜„ì§€ íŒ¬ë“¤ì˜ ìƒìƒí•œ ë°˜ì‘ì„ ë‰´ìŠ¤ í¬ë§·ì— ë§ê²Œ 1ë¬¸ë‹¨ìœ¼ë¡œ ì¬ë¯¸ìˆê²Œ ìš”ì•½ (í•œêµ­ì–´). ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´",
        "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
    }}

    Article Body:
    {truncated_text}
    """

    # ì¬ì‹œë„ ë¡œì§ (Exponential Backoff)
    max_retries = 3
    base_delay = 5  # 5ì´ˆë¶€í„° ì‹œì‘

    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
            return json.loads(response.text)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                wait_time = base_delay * (2 ** attempt) + (attempt * 2) # Jitter ì¶”ê°€
                print(f"âš ï¸ Quota exceeded. Retrying in {wait_time}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ Summary failed (API Error): {e}")
                break
    
    return {
        "title_en": title,
        "summary_en": "Summarization failed.",
        "title_kr": title,
        "summary_kr": "ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "content_kr": "ë³¸ë¬¸ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
        "keywords": []
    }

def fetch_broadway_grosses():
    """Playbill.comì—ì„œ ì´ë²ˆ ì£¼ ë¸Œë¡œë“œì›¨ì´ ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = "https://www.playbill.com/grosses"
    headers = {"User-Agent": REDDIT_USER_AGENT}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = bs4.BeautifulSoup(resp.content, 'html.parser')
            table = soup.find('table')
            if not table:
                return []
            
            rows = table.find('tbody').find_all('tr') if table.find('tbody') else table.find_all('tr')[1:]
            grosses = []
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 8:
                    # ê³µì—°ëª…ê³¼ ê·¹ì¥ëª… ë¶„ë¦¬
                    full_text = cols[0].get_text(strip=True)
                    show_node = cols[0].find('a')
                    show_name = show_node.get_text(strip=True) if show_node else full_text
                    # ê·¹ì¥ëª…: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê³µì—°ëª…ì„ ë¹¼ë©´ ê·¹ì¥ëª…ë§Œ ë‚¨ìŒ
                    theater_name = full_text.replace(show_name, '').strip()
                    
                    gross_str = cols[1].get_text(strip=True)
                    diff_str = cols[2].get_text(strip=True)  # ì „ì£¼ ëŒ€ë¹„ ë³€ë™
                    avg_ticket_str = cols[3].get_text(strip=True)  # í‰ê·  í‹°ì¼“ ê°€ê²©
                    attendance_str = cols[4].get_text(strip=True)  # ê´€ê° ìˆ˜
                    capacity_str = cols[6].get_text(strip=True)
                    
                    try:
                        parsed_gross = float(gross_str.replace('$', '').replace(',', ''))
                        grosses.append({
                            "show": show_name,
                            "theater": theater_name,
                            "gross_formatted": gross_str,
                            "gross": parsed_gross,
                            "diff": diff_str,
                            "avg_ticket": avg_ticket_str,
                            "attendance": attendance_str,
                            "capacity": capacity_str
                        })
                    except ValueError:
                        continue
                        
            # ë§¤ì¶œì•¡(Gross) ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ ì¶”ì¶œ
            grosses.sort(key=lambda x: x['gross'], reverse=True)
            top5 = grosses[:5]
            for i, item in enumerate(top5):
                item['rank'] = i + 1
            
            # LLMìœ¼ë¡œ ê° ê³µì—°ì— ëŒ€í•œ í•œ ì¤„ í•œêµ­ì–´ ì†Œê°œ ì¶”ê°€
            if GEMINI_API_KEY and top5:
                try:
                    show_list = ", ".join([s['show'] for s in top5])
                    desc_prompt = f"""ì•„ë˜ 5ê°œ ë¸Œë¡œë“œì›¨ì´ ê³µì—°ì— ëŒ€í•´ ê°ê° í•œêµ­ì–´ë¡œ í•œ ì¤„(15~25ì) ì†Œê°œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë®¤ì§€ì»¬ì´ë©´ ì¥ë¥´ì™€ ë¶„ìœ„ê¸°ë¥¼, ì—°ê·¹ì´ë©´ ì£¼ì œë‚˜ ë°°ìš°ë¥¼ ê°„ë‹¨íˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ JSON ê°ì²´ í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. í‚¤ëŠ” ê³µì—°ëª…(ì˜ë¬¸), ê°’ì€ í•œêµ­ì–´ ì†Œê°œì…ë‹ˆë‹¤.

ê³µì—° ëª©ë¡: {show_list}

ì˜ˆì‹œ: {{"Hamilton": "ë¯¸êµ­ ê±´êµ­ì˜ ì—­ì‚¬ë¥¼ í™í•©ìœ¼ë¡œ í’€ì–´ë‚¸ ë®¤ì§€ì»¬"}}
"""
                    desc_resp = model.generate_content(desc_prompt).text.strip()
                    if desc_resp.startswith("```json"):
                        desc_resp = desc_resp[7:]
                    if desc_resp.endswith("```"):
                        desc_resp = desc_resp[:-3]
                    descriptions = json.loads(desc_resp.strip())
                    for item in top5:
                        item['description_kr'] = descriptions.get(item['show'], '')
                except Exception as e:
                    print(f"   âš ï¸ ê³µì—° ì†Œê°œ ìƒì„± ìŠ¤í‚µ: {e}")
            
            return top5
    except Exception as e:
        print(f"   âš ï¸ ë¸Œë¡œë“œì›¨ì´ ë°•ìŠ¤ì˜¤í”¼ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
    return []

def generate_weekly_recommendations(articles_data):
    """ìµœê·¼ ì¸ë”” ë§¤ì²´ ì¤‘ì‹¬ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´/ì‹œì¹´ê³  ì¶”ì²œì‘ 3ê°œë¥¼ ë½‘ìŠµë‹ˆë‹¤."""
    if not GEMINI_API_KEY or not articles_data:
        return []

    context = ""
    for idx, article in enumerate(articles_data[:10]):
        context += f"[{idx+1}] ì œëª©: {article['title_kr']}\nìš”ì•½: {article['summary_kr']}\n\n"
        
    prompt = f"""
    ë‹¹ì‹ ì€ ë¯¸êµ­ ì—°ê·¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì´ë²ˆ ì£¼ ìˆ˜ì§‘ëœ ì—°ê·¹ ê¸°ì‚¬ë“¤ì˜ ëª©ë¡ì…ë‹ˆë‹¤.
    ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´ ë˜ëŠ” ì‹œì¹´ê³  ë“± ì§€ì—­ ì—°ê·¹/í™”ì œì‘ ì¤‘ ì¶”ì²œí•  ë§Œí•œ ì‘í’ˆ 3ê°œ**ë¥¼ ì„ ì •í•´ ì§§ê²Œ ì¶”ì²œ ì´ìœ ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ê¸°ì‚¬ ëª©ë¡:
    {context}

    ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”. ê° ê°ì²´ëŠ” "title" (ì‘í’ˆëª…, í•œê¸€/ì˜ë¬¸ ë³‘ê¸°), "reason" (ì¶”ì²œ ì´ìœ , 2ë¬¸ì¥ ì´ë‚´) í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
    ê¸°ì‚¬ ë‚´ìš© ì¤‘ ì¶”ì²œí•  ë§Œí•œ êµ¬ì²´ì ì¸ ì—°ê·¹ ì‘í’ˆì´ ë¶€ì¡±í•˜ë‹¤ë©´, í˜„ì¬ ë¯¸êµ­ì—ì„œ í‰ë‹¨ì˜ ë†’ì€ ì§€ì§€ë¥¼ ë°›ê³  ìˆëŠ” ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´ í™”ì œì‘ì„ ì„ì˜ë¡œ ê³¨ë¼ë„ ì¢‹ìŠµë‹ˆë‹¤.
    
    [
        {{"title": "ë¯¼ì¤‘ì˜ ì  (An Enemy of the People)", "reason": "ì œë ˆë¯¸ ìŠ¤íŠ¸ë¡±ì˜ ëª…ì—°ê¸°ì™€ í•¨ê»˜ í™˜ê²½ ë¬¸ì œë¼ëŠ” ì‹œëŒ€ì  í™”ë‘ë¥¼ ë˜ì§€ëŠ” í•„ëŒ ì—°ê·¹ì…ë‹ˆë‹¤."}}
    ]
    ë‹µë³€ì€ ì˜¤ì§ JSON í˜•ì‹ìœ¼ë¡œë§Œ í•´ì£¼ì„¸ìš”.
    """
    try:
        response = model.generate_content(prompt).text.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.endswith("```"):
            response = response[:-3]
        return json.loads(response.strip())
    except Exception as e:
        print(f"   âš ï¸ ì£¼ê°„ ì¶”ì²œì‘ ìƒì„± ì‹¤íŒ¨: {e}")
    return []

def process_entry(entry, source, tier):
    """Process individual article (for parallel execution)"""
    title = entry.title
    link = entry.link
    published = entry.get('published', datetime.now().strftime("%Y-%m-%d"))
    
    print(f"   Analyzing [{tier.upper()}]: {title[:30]}...")

    # 1. Extract full text + image from article HTML
    full_text, html_image = fetch_article_content(link)
    
    # 2. Extract local reactions from Reddit (URL & Title based)
    reddit_comments = fetch_reddit_comments(link, title, [])
    
    # 3. AI Summary with Reddit context
    ai_result = translate_and_summarize(full_text, title, reddit_comments)

    # Extract image: RSS metadata ìš°ì„ , ì—†ìœ¼ë©´ HTML íŒŒì‹±, ê·¸ë˜ë„ ì—†ìœ¼ë©´ Wikipedia ê²€ìƒ‰
    image_url = html_image  # ê¸°ë³¸ê°’: HTMLì—ì„œ ì¶”ì¶œí•œ ì´ë¯¸ì§€
    if 'media_content' in entry and len(entry.media_content) > 0:
        image_url = entry.media_content[0].get('url', '') or html_image
    elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
        image_url = entry.media_thumbnail[0].get('url', '') or html_image
    elif 'links' in entry:
        for link_item in entry.links:
            if link_item.get('type', '').startswith('image/'):
                image_url = link_item.get('href', '') or html_image
                break

    # RSSë‚˜ HTMLì—ì„œ ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ Wikipedia ì´ë¯¸ì§€ ê²€ìƒ‰
    if not image_url and ai_result.get('keywords'):
        # AIê°€ ì¶”ì¶œí•œ ì˜ë¬¸ í‚¤ì›Œë“œë¡œ ì§ì ‘ ê²€ìƒ‰ (ì›ë¬¸ ì œëª©ì—ì„œ ë¡œë§ˆì ì°¾ê¸°)
        import re
        # ì˜ë¬¸ ë‹¨ì–´ê°€ í¬í•¨ëœ í‚¤ì›Œë“œ ìš°ì„  (ex. ë°°ìš° ì´ë¦„ ë“±)
        original_title_words = entry.title
        en_keywords = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+', original_title_words)
        search_keywords = en_keywords + ai_result.get('keywords', [])
        image_url = fetch_wikipedia_image(search_keywords)

    return {
        "source": source,
        "tier": tier,  # 'major' ë˜ëŠ” 'indie'
        "original_title": title,
        "link": link,
        "image": image_url,
        "title_kr": ai_result.get('title_kr', title),
        "summary_kr": ai_result.get('summary_kr', 'ë‚´ìš© ì—†ìŒ'),
        "content_kr": ai_result.get('content_kr', 'ë‚´ìš© ì—†ìŒ'),
        "reddit_reaction_kr": ai_result.get('reddit_reaction_kr', ''),
        "keywords": ai_result.get('keywords', []),
        "date": published,
        "scraped_at": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S") # Changed to timezone.utc
    }

def send_email(articles):
    if not EMAIL_SENDER or not EMAIL_PASSWORD or not articles:
        return

    subject = f"[StageSide] Latest News Briefing - {datetime.now().strftime('%Y-%m-%d %H:%M')}" # Kept datetime.now() as per instruction for subject
    body = "<h2>Today's Top News</h2><br>"
    
    for article in articles:
        body += f"<h3>[{article['source']}] {article['title_kr']}</h3>"
        body += f"<p>{article['summary_kr']}</p>"
        body += f"<p><small>Keywords: {', '.join(article['keywords'])}</small></p>"
        body += f"<p><a href='{article['link']}' target='_blank'>Read Original</a></p><hr>"

    msg = MIMEText(body, 'html')
    msg['Subject'] = subject
    msg['From'] = EMAIL_SENDER
    msg['To'] = EMAIL_RECEIVER

    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(EMAIL_SENDER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë ˆí¬íŠ¸ ë°œì†¡ ì™„ë£Œ ({len(articles)}ê±´)")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def save_to_json(major_articles, indie_articles):
    file_path = 'data/articles.json'
    
    # ë³¼ë¥¨ í™•ëŒ€: ë©”ì´ì € 5ê°œ + ì¸ë”” 4ê°œ ìœ ì§€ (ì´ 9ê°œ)
    final_data = major_articles[:5] + indie_articles[:4]

    os.makedirs("data", exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: ë©”ì´ì € {len(major_articles[:5])}ê±´ + ì¸ë”” {len(indie_articles[:4])}ê±´ = ì´ {len(final_data)}ê±´")
    
    # Sitemap ìƒì„± ë¡œì§ ì¶”ê°€
    generate_sitemap(final_data)

def generate_sitemap(articles):
    """
    ì €ì¥ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ê¸€ ê²€ìƒ‰ìš© sitemap.xmlì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    import urllib.parse
    base_url = "https://limjinou.github.io/collectivemonologue"
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    xml_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
    xml_content += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    # ê¸°ë³¸ ì •ì  í˜ì´ì§€ë“¤
    static_pages = ['index.html', 'category.html', 'about.html', 'contact.html', 'privacy.html']
    for page in static_pages:
        xml_content += '  <url>\n'
        xml_content += f'    <loc>{base_url}/{page}</loc>\n'
        xml_content += f'    <lastmod>{today}</lastmod>\n'
        xml_content += '    <changefreq>daily</changefreq>\n'
        xml_content += '    <priority>0.8</priority>\n'
        xml_content += '  </url>\n'

    # ë™ì  ê¸°ì‚¬ í˜ì´ì§€ë“¤ (article.html?id=...)
    for article in articles:
        # ì˜ë¬¸ íƒ€ì´í‹€ì„ ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ íŒŒì‹±í•˜ì—¬ ID ìƒì„± (frontend main.jsì™€ ë™ì¼ ë¡œì§ ì˜ˆìƒ)
        safe_title = "".join(c if c.isalnum() or c.isspace() else "" for c in article['original_title']).strip()
        article_id = safe_title.replace(' ', '_').lower()
        encoded_id = urllib.parse.quote(article_id)
        
        xml_content += '  <url>\n'
        xml_content += f'    <loc>{base_url}/article.html?id={encoded_id}</loc>\n'
        xml_content += f'    <lastmod>{today}</lastmod>\n'
        xml_content += '    <changefreq>weekly</changefreq>\n'
        xml_content += '    <priority>0.6</priority>\n'
        xml_content += '  </url>\n'

    xml_content += '</urlset>'

    with open('sitemap.xml', 'w', encoding='utf-8') as f:
        f.write(xml_content)
    print("âœ… sitemap.xml ìë™ ìƒì„± ì™„ë£Œ")

def crawl_rss():
    print("ğŸš€ í¬ë¡¤ëŸ¬(ver.2) ì‹œì‘ â€” ë©”ì´ì € 5ê±´ + ì¸ë”” 4ê±´ ìˆ˜ì§‘")
    
    def fetch_from_feeds(feeds_dict, tier):
        entries = []
        for source, url in feeds_dict.items():
            print(f"ğŸ“¡ [{tier.upper()}] {source} ê²€ìƒ‰ ì¤‘...")
            try:
                feed = feedparser.parse(url)
                # ê° ì†ŒìŠ¤ë³„ ìµœì‹  2ê°œì”© ìˆ˜ì§‘ (ë²„í¼ í™•ë³´: 1ê°œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ê²ƒìœ¼ë¡œ ëŒ€ì²´)
                for entry in feed.entries[:2]:
                    entries.append((entry, source, tier))
            except Exception as e:
                print(f"âš ï¸ {source} í”¼ë“œ ì˜¤ë¥˜: {e}")
        return entries

    major_entries = fetch_from_feeds(MAJOR_FEEDS, 'major')
    indie_entries = fetch_from_feeds(INDIE_FEEDS, 'indie')
    all_entries = major_entries + indie_entries
    
    print(f"ì´ {len(all_entries)}ê°œ ê¸°ì‚¬ ë°œê²¬. ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

    major_results = []
    indie_results = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_entry = {
            executor.submit(process_entry, entry, source, tier): (entry, source, tier)
            for entry, source, tier in all_entries
        }
        for future in concurrent.futures.as_completed(future_to_entry):
            try:
                data = future.result()
                if data['tier'] == 'major':
                    major_results.append(data)
                else:
                    indie_results.append(data)
            except Exception as exc:
                print(f"âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {exc}")

    return major_results, indie_results

if __name__ == "__main__":
    major_data, indie_data = crawl_rss()
    if major_data or indie_data:
        save_to_json(major_data, indie_data)
        all_data = major_data + indie_data
        send_email(all_data)
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
