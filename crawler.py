import feedparser
import smtplib
from email.mime.text import MIMEText
import os
import google.generativeai as genai
from datetime import datetime
from dotenv import load_dotenv
import json
import trafilatura
import concurrent.futures
import time

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
EMAIL_SENDER = os.getenv("EMAIL_SENDER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.getenv("EMAIL_RECEIVER")

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # Flash Latest ì‚¬ìš© (Stable version, quota friendly)
    model = genai.GenerativeModel('gemini-flash-latest')

RSS_FEEDS = {
    # ì—°ê·¹ íŠ¹í™” ì†ŒìŠ¤ë§Œ ìœ ì§€ (í˜„ì¬ Playbill RSS ë“± ì¼ë¶€ í”¼ë“œê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆì–´ ë³µìˆ˜ë¡œ ì¶”ê°€)
    "Deadline Theater": "https://deadline.com/v/theater/feed/",
    "Playbill": "https://www.playbill.com/rss",
    "BroadwayWorld": "https://www.broadwayworld.com/rss/news.xml"
}

def fetch_article_content(url):
    """Trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded)
            return text
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
    return None

def translate_and_summarize(text, title):
    if not GEMINI_API_KEY:
        return {"title_en": title, "summary_en": "No API Key provided.", "keywords": []}

    if not text or len(text) < 50:
        return {"title_en": title, "summary_en": "Content too short or extraction failed.", "keywords": []}
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë³´ëƒ„ (í† í° ì œí•œ ë°©ì§€)
    truncated_text = text[:4000]

    prompt = f"""
    Here is an article about '{title}'.
    Please extract the core information and rewrite it in Korean.
    The goal is to provide a professional summary of the hottest issues, highly anticipated shows, or upcoming works in the US theater scene.
    
    Format the output as the following JSON. 
    Make sure to write 'title_kr', 'summary_kr', and 'content_kr' in KOREAN:
    {{
        "title_kr": "í•œêµ­ì–´ë¡œ ë²ˆì—­/ê°ìƒ‰ëœ ê¸°ì‚¬ ì œëª©",
        "summary_kr": "ë¦¬ìŠ¤íŠ¸ ë©”ì¸ í™”ë©´ì— ë“¤ì–´ê°ˆ 1-2ë¬¸ì¥ì˜ í¥ë¯¸ë¡œìš´ ìš”ì•½ (í•œêµ­ì–´)",
        "content_kr": "ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš©. ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì–´ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„± (í•œêµ­ì–´).",
        "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
    }}

    Article Body:
    {truncated_text}
    """

    # ì¬ì‹œë„ ë¡œì§ (Exponential Backoff)
    max_retries = 3
    base_delay = 5  # 5ì´ˆë¶€í„° ì‹œì‘

    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
            return json.loads(response.text)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                wait_time = base_delay * (2 ** attempt) + (attempt * 2) # Jitter ì¶”ê°€
                print(f"âš ï¸ Quota exceeded. Retrying in {wait_time}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ Summary failed (API Error): {e}")
                break
    
    return {
        "title_kr": title,
        "summary_kr": "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "content_kr": "ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "keywords": []
    }

def process_entry(entry, source):
    """Process individual article (for parallel execution)"""
    title = entry.title
    link = entry.link
    published = entry.get('published', datetime.now().strftime("%Y-%m-%d"))
    
    print(f"   Analyzing: {title[:30]}...")

    # 1. Extract full text
    full_text = fetch_article_content(link)
    
    # 2. AI Summary
    ai_result = translate_and_summarize(full_text, title)

    # Extract image from entry or feed
    image_url = ""
    # Try different common RSS image enclosures
    if 'media_content' in entry and len(entry.media_content) > 0:
        image_url = entry.media_content[0].get('url', '')
    elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
        image_url = entry.media_thumbnail[0].get('url', '')
    elif 'links' in entry:
        for link_item in entry.links:
            if link_item.get('type', '').startswith('image/'):
                image_url = link_item.get('href', '')
                break

    return {
        "source": source,
        "original_title": title,
        "link": link,
        "image": image_url,
        "title_kr": ai_result.get('title_kr', title),
        "summary_kr": ai_result.get('summary_kr', 'ë‚´ìš© ì—†ìŒ'),
        "content_kr": ai_result.get('content_kr', 'ë‚´ìš© ì—†ìŒ'),
        "keywords": ai_result.get('keywords', []),
        "date": published,
        "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

def send_email(articles):
    if not EMAIL_SENDER or not EMAIL_PASSWORD or not articles:
        return

    subject = f"[StageSide] Latest News Briefing - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    body = "<h2>Today's Top News</h2><br>"
    
    for article in articles:
        body += f"<h3>[{article['source']}] {article['title_kr']}</h3>"
        body += f"<p>{article['summary_kr']}</p>"
        body += f"<p><small>Keywords: {', '.join(article['keywords'])}</small></p>"
        body += f"<p><a href='{article['link']}' target='_blank'>Read Original</a></p><hr>"

    msg = MIMEText(body, 'html')
    msg['Subject'] = subject
    msg['From'] = EMAIL_SENDER
    msg['To'] = EMAIL_RECEIVER

    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(EMAIL_SENDER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë ˆí¬íŠ¸ ë°œì†¡ ì™„ë£Œ ({len(articles)}ê±´)")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def save_to_json(new_data):
    file_path = 'data/articles.json'
    
    # í”„ë¡œí† íƒ€ì… ë‹¨ê³„: ëª¨ë“  ê¸°ì¡´ ë°ì´í„° ì§€ìš°ê³  ìƒˆë¡œ ê°€ì ¸ì˜¨ ë”± 2ê°œë§Œ ìœ ì§€
    final_data = new_data[:2]

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… ë°ì´í„° ë®ì–´ì“°ê¸° ì™„ë£Œ (ì´ {len(final_data)}ê±´ì˜ í•«ì´ìŠˆ ê¸°ì‚¬ ìœ ì§€)")

def crawl_rss():
    print("ğŸš€ ê³ ì„±ëŠ¥ í¬ë¡¤ëŸ¬(ver.1) ì‹œì‘...")
    
    all_entries = []
    for source, url in RSS_FEEDS.items():
        print(f"ğŸ“¡ {source} ê²€ìƒ‰ ì¤‘...")
        feed = feedparser.parse(url)
        # ê° ì†ŒìŠ¤ë³„ ìµœì‹  2ê°œë§Œ ìˆ˜ì§‘
        for entry in feed.entries[:2]:
            all_entries.append((entry, source))
    
    print(f"ì´ {len(all_entries)}ê°œì˜ ê¸°ì‚¬ ë°œê²¬. ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

    results = []
    # ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 2ê°œ ë™ì‹œ ì‘ì—… - Rate Limit ë°©ì§€)
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_entry = {executor.submit(process_entry, entry, source): (entry, source) for entry, source in all_entries}
        for future in concurrent.futures.as_completed(future_to_entry):
            try:
                data = future.result()
                results.append(data)
            except Exception as exc:
                print(f"âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {exc}")

    return results

if __name__ == "__main__":
    crawled_data = crawl_rss()
    if crawled_data:
        save_to_json(crawled_data)
        send_email(crawled_data)
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
