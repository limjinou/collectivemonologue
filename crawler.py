import feedparser
import smtplib
from email.mime.text import MIMEText
import os
import google.generativeai as genai
from datetime import datetime
from dotenv import load_dotenv
import json
import trafilatura
import concurrent.futures
import time

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
EMAIL_SENDER = os.getenv("EMAIL_SENDER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.getenv("EMAIL_RECEIVER")

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # Flash Latest ì‚¬ìš© (Stable version, quota friendly)
    model = genai.GenerativeModel('gemini-flash-latest')

# ë©”ì´ì € ì†ŒìŠ¤ (ë¸Œë¡œë“œì›¨ì´ / í• ë¦¬ìš°ë“œ ë©”ì´ì €)
MAJOR_FEEDS = {
    "Playbill": "https://www.playbill.com/rss",
    "BroadwayWorld": "https://www.broadwayworld.com/rss/news.xml",
    "Deadline Theater": "https://deadline.com/v/theater/feed/",
}

# ì¸ë”” ì†ŒìŠ¤ (ëŒ€í•™ë¡œ ê°ì„±, ë¹„ì˜ë¦¬, ì†Œê·œëª¨ ê·¹ì¥)
INDIE_FEEDS = {
    "American Theatre": "https://www.americantheatre.org/feed/",
    "TheaterMania": "https://www.theatermania.com/rss",
}

def fetch_article_content(url):
    """Trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded)
            return text
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
    return None

def translate_and_summarize(text, title):
    if not GEMINI_API_KEY:
        return {"title_en": title, "summary_en": "No API Key provided.", "keywords": []}

    if not text or len(text) < 50:
        return {"title_en": title, "summary_en": "Content too short or extraction failed.", "keywords": []}
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë³´ëƒ„ (í† í° ì œí•œ ë°©ì§€)
    truncated_text = text[:4000]

    prompt = f"""
    You are the editor of "Collective Monologue", a Korean-language magazine dedicated to covering American theater and film with depth, nuance, and cultural context.
    
    Below is an article titled '{title}'. Your task is NOT a simple translation.
    Instead, produce a rich, original Korean editorial that:

    1. Summarizes the core news from the article
    2. Adds meaningful background knowledge YOU ALREADY KNOW about:
       - Any ACTORS or DIRECTORS mentioned: their notable past works, career highlights, and what makes them significant
       - Any PRODUCTIONS or PLAYS mentioned: the original playwright, a brief synopsis, the work's historical/cultural significance
       - Any THEATERS or VENUES mentioned: their location, founding history, notable past productions, or their role in American theater
       - Any AWARDS or EVENTS mentioned: the history and significance of the award or event
    3. Includes a brief editorial perspective or "editor's note" that helps Korean readers understand WHY this news matters in the context of American theater/film culture

    Write as a knowledgeable Korean cultural journalist â€” warm, insightful, and informative.
    The output must be a JSON object with KOREAN text for title_kr, summary_kr, and content_kr:
    {{
        "title_kr": "í•œêµ­ ë…ìì˜ í¥ë¯¸ë¥¼ ëŒ ìˆ˜ ìˆëŠ” ë§¤ë ¥ì ì¸ ê¸°ì‚¬ ì œëª© (í•œêµ­ì–´)",
        "summary_kr": "ë©”ì¸ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ì— í‘œì‹œë  1-2ë¬¸ì¥ì˜ í•µì‹¬ ìš”ì•½. ë…ìê°€ í´ë¦­í•˜ê³  ì‹¶ê²Œ ë§Œë“¤ì–´ë¼ (í•œêµ­ì–´)",
        "content_kr": "ê¸°ì‚¬ ë³¸ë¬¸. ë‰´ìŠ¤ ìš”ì•½ + ë“±ì¥ ì¸ë¬¼/ì‘í’ˆ/ê³µì—°ì¥ì— ëŒ€í•œ ë°°ê²½ ì§€ì‹ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì¸ í’ë¶€í•œ í…ìŠ¤íŠ¸. ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì–´ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„±. ë§ˆì§€ë§‰ì—” 'í¸ì§‘ì ì£¼' ë˜ëŠ” í•œêµ­ ë…ìë¥¼ ìœ„í•œ ë§¥ë½ ì„¤ëª… í•œ ë¬¸ë‹¨ì„ ì¶”ê°€í•  ê²ƒ (í•œêµ­ì–´)",
        "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
    }}

    Article Body:
    {truncated_text}
    """

    # ì¬ì‹œë„ ë¡œì§ (Exponential Backoff)
    max_retries = 3
    base_delay = 5  # 5ì´ˆë¶€í„° ì‹œì‘

    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
            return json.loads(response.text)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                wait_time = base_delay * (2 ** attempt) + (attempt * 2) # Jitter ì¶”ê°€
                print(f"âš ï¸ Quota exceeded. Retrying in {wait_time}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ Summary failed (API Error): {e}")
                break
    
    return {
        "title_kr": title,
        "summary_kr": "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "content_kr": "ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "keywords": []
    }

def process_entry(entry, source, tier):
    """Process individual article (for parallel execution)"""
    title = entry.title
    link = entry.link
    published = entry.get('published', datetime.now().strftime("%Y-%m-%d"))
    
    print(f"   Analyzing [{tier.upper()}]: {title[:30]}...")

    # 1. Extract full text
    full_text = fetch_article_content(link)
    
    # 2. AI Summary
    ai_result = translate_and_summarize(full_text, title)

    # Extract image from entry or feed
    image_url = ""
    # Try different common RSS image enclosures
    if 'media_content' in entry and len(entry.media_content) > 0:
        image_url = entry.media_content[0].get('url', '')
    elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
        image_url = entry.media_thumbnail[0].get('url', '')
    elif 'links' in entry:
        for link_item in entry.links:
            if link_item.get('type', '').startswith('image/'):
                image_url = link_item.get('href', '')
                break

    return {
        "source": source,
        "tier": tier,  # 'major' ë˜ëŠ” 'indie'
        "original_title": title,
        "link": link,
        "image": image_url,
        "title_kr": ai_result.get('title_kr', title),
        "summary_kr": ai_result.get('summary_kr', 'ë‚´ìš© ì—†ìŒ'),
        "content_kr": ai_result.get('content_kr', 'ë‚´ìš© ì—†ìŒ'),
        "keywords": ai_result.get('keywords', []),
        "date": published,
        "scraped_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

def send_email(articles):
    if not EMAIL_SENDER or not EMAIL_PASSWORD or not articles:
        return

    subject = f"[StageSide] Latest News Briefing - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    body = "<h2>Today's Top News</h2><br>"
    
    for article in articles:
        body += f"<h3>[{article['source']}] {article['title_kr']}</h3>"
        body += f"<p>{article['summary_kr']}</p>"
        body += f"<p><small>Keywords: {', '.join(article['keywords'])}</small></p>"
        body += f"<p><a href='{article['link']}' target='_blank'>Read Original</a></p><hr>"

    msg = MIMEText(body, 'html')
    msg['Subject'] = subject
    msg['From'] = EMAIL_SENDER
    msg['To'] = EMAIL_RECEIVER

    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(EMAIL_SENDER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë ˆí¬íŠ¸ ë°œì†¡ ì™„ë£Œ ({len(articles)}ê±´)")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def save_to_json(major_articles, indie_articles):
    file_path = 'data/articles.json'
    
    # ë©”ì´ì € 2ê°œ + ì¸ë”” 2ê°œ ìœ ì§€
    final_data = major_articles[:2] + indie_articles[:2]

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: ë©”ì´ì € {len(major_articles[:2])}ê±´ + ì¸ë”” {len(indie_articles[:2])}ê±´ = ì´ {len(final_data)}ê±´")

def crawl_rss():
    print("ğŸš€ í¬ë¡¤ëŸ¬(ver.2) ì‹œì‘ â€” ë©”ì´ì € 2ê±´ + ì¸ë”” 2ê±´ ìˆ˜ì§‘")
    
    def fetch_from_feeds(feeds_dict, tier):
        entries = []
        for source, url in feeds_dict.items():
            print(f"ğŸ“¡ [{tier.upper()}] {source} ê²€ìƒ‰ ì¤‘...")
            try:
                feed = feedparser.parse(url)
                # ê° ì†ŒìŠ¤ë³„ ìµœì‹  1ê°œì”© ìˆ˜ì§‘
                for entry in feed.entries[:1]:
                    entries.append((entry, source, tier))
            except Exception as e:
                print(f"âš ï¸ {source} í”¼ë“œ ì˜¤ë¥˜: {e}")
        return entries

    major_entries = fetch_from_feeds(MAJOR_FEEDS, 'major')
    indie_entries = fetch_from_feeds(INDIE_FEEDS, 'indie')
    all_entries = major_entries + indie_entries
    
    print(f"ì´ {len(all_entries)}ê°œ ê¸°ì‚¬ ë°œê²¬. ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

    major_results = []
    indie_results = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_entry = {
            executor.submit(process_entry, entry, source, tier): (entry, source, tier)
            for entry, source, tier in all_entries
        }
        for future in concurrent.futures.as_completed(future_to_entry):
            try:
                data = future.result()
                if data['tier'] == 'major':
                    major_results.append(data)
                else:
                    indie_results.append(data)
            except Exception as exc:
                print(f"âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {exc}")

    return major_results, indie_results

if __name__ == "__main__":
    major_data, indie_data = crawl_rss()
    if major_data or indie_data:
        save_to_json(major_data, indie_data)
        all_data = major_data + indie_data
        send_email(all_data)
    else:
        print("ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
