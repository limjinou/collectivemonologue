import feedparser
import smtplib
from email.mime.text import MIMEText
import os
import google.generativeai as genai
from datetime import datetime, timezone
from dotenv import load_dotenv
import json
import trafilatura
import concurrent.futures
import time
import requests
import bs4

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ë° ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
EMAIL_SENDER = os.getenv("EMAIL_SENDER")
EMAIL_PASSWORD = os.getenv("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.getenv("EMAIL_RECEIVER")
REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT", "CollectiveMonologue_Crawler/1.0")

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    # ì•ˆì •ì ì¸ Quotaë¥¼ ì œê³µí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” Gemini Pro Latest ëª¨ë¸ ì‚¬ìš©
    model = genai.GenerativeModel('gemini-pro-latest')

# ë©”ì´ì € ì†ŒìŠ¤ (ë¸Œë¡œë“œì›¨ì´ / í• ë¦¬ìš°ë“œ ë©”ì´ì €)
MAJOR_FEEDS = {
    "Playbill": "https://www.playbill.com/rss",
    "BroadwayWorld": "https://www.broadwayworld.com/rss/news.xml",
    "Deadline Theater": "https://deadline.com/v/theater/feed/",
    "The Hollywood Reporter": "https://www.hollywoodreporter.com/feed/",
    "IndieWire": "https://www.indiewire.com/feed/",
    "Variety Theater": "https://variety.com/v/legit/feed/",
}

# ì¸ë”” ì†ŒìŠ¤ (ëŒ€í•™ë¡œ ê°ì„±, ë¹„ì˜ë¦¬, ì†Œê·œëª¨ ê·¹ì¥)
INDIE_FEEDS = {
    "American Theatre": "https://www.americantheatre.org/feed/",
    "HowlRound": "https://howlround.com/rss.xml",  # ì˜¨ë¼ì¸ ë¹„ì˜ë¦¬ ì—°ê·¹ ë§¤ê±°ì§„ HowlRound
    "TheaterMania": "https://www.theatermania.com/feed/",
    "Backstage": "https://www.backstage.com/magazine/article/feed/",
}

def fetch_article_content(url):
    """Trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë³¸ë¬¸ ë° ê³ í•´ìƒë„ ì´ë¯¸ì§€ URL ì¶”ì¶œ (og:image ìš°ì„ )"""
    try:
        image_url = ""
        try:
            resp = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"})
            if resp.status_code == 200:
                soup = bs4.BeautifulSoup(resp.content, 'html.parser')
                og_img = soup.find('meta', property='og:image')
                if og_img and og_img.get('content'):
                    image_url = og_img['content']
                elif soup.find('meta', attrs={'name': 'twitter:image'}):
                    image_url = soup.find('meta', attrs={'name': 'twitter:image'})['content']
        except Exception:
            pass

        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            text = trafilatura.extract(downloaded)
            # og:imageë¥¼ ëª» ì°¾ì•˜ì„ ê²½ìš° ë³¸ë¬¸ì—ì„œ ì²« ë²ˆì§¸ img íƒœê·¸ ì¶”ì¶œ ì‹œë„
            if not image_url:
                import re
                img_match = re.search(r'<img[^>]+src=["\']([^"\'>]+)["\']', downloaded)
                if img_match:
                    candidate = img_match.group(1)
                    # ëª…ë°±í•œ ë”ë¯¸ ì´ë¯¸ì§€, ë¹ˆ í”½ì…€, ì¶”ì  í”½ì…€ ë“± ì² ì €íˆ ì œì™¸
                    if candidate.startswith('http') and not any(x in candidate.lower() for x in ['logo', 'icon', 'avatar', 'pixel', '1x1', 'blank', 'scorecardresearch']):
                        image_url = candidate
            return text, image_url
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
    return None, ""

def fetch_reddit_comments(article_url, title, keywords):
    """Reddit ì„œë¸Œë ˆë”§ Hot ê²Œì‹œë¬¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë°˜ì‘ ìˆ˜ì§‘ (í‚¤ì›Œë“œ ë§¤ì¹­ ë°©ì‹)"""
    headers = {"User-Agent": REDDIT_USER_AGENT}

    def get_comments(post_id):
        url = f"https://www.reddit.com/comments/{post_id}.json?sort=confidence&limit=5"
        comments = []
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                if len(data) > 1:
                    comment_children = data[1].get('data', {}).get('children', [])
                    for child in comment_children:
                        if child['kind'] == 't1':
                            body = child['data'].get('body', '')
                            if len(body) > 10 and "[deleted]" not in body:
                                comments.append(body.replace('\n', ' '))
        except Exception:
            pass
        return comments

    def fetch_subreddit_hot(subreddit, limit=15):
        """ì„œë¸Œë ˆë”§ì˜ ìµœì‹ (Hot) ê²Œì‹œë¬¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                children = resp.json().get('data', {}).get('children', [])
                return [c['data'] for c in children]
        except Exception:
            pass
        return []

    import re
    try:
        # ê¸°ì‚¬ ì œëª©ì˜ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ì§§ì€ ë‹¨ì–´ ë° ë¶ˆìš©ì–´ ì œê±°)
        stopwords = {'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'is', 'are', 'was', 'will', 'with', 'that', 'this', 'from', 'by', 'as', 'it', 'new', 'its', 'into', 'has', 'have', 'set'}
        title_words = set(w.lower() for w in re.findall(r'\b[A-Za-z]{4,}\b', title) if w.lower() not in stopwords)

        subreddits_to_try = ['Broadway', 'theater', 'movies', 'boxoffice']
        best_post = None
        best_score = 0

        for subreddit in subreddits_to_try:
            posts = fetch_subreddit_hot(subreddit, limit=15)
            time.sleep(0.5)
            for post in posts:
                post_title = post.get('title', '')
                post_words = set(w.lower() for w in re.findall(r'\b[A-Za-z]{4,}\b', post_title))
                overlap = len(title_words & post_words)
                if overlap > best_score:
                    best_score = overlap
                    best_post = post
                    best_post['_subreddit'] = subreddit

        # ê³µí†µ í‚¤ì›Œë“œ 1ê°œ ì´ìƒì´ë©´ ê´€ë ¨ ê²Œì‹œë¬¼ë¡œ ì¸ì • (ì—„ê²© ê¸°ì¤€ë³´ë‹¤ ìˆ˜ì§‘ë¥  ìš°ì„ )
        if best_post and best_score >= 1:
            post_id = best_post.get('id')
            subreddit_name = best_post.get('_subreddit', best_post.get('subreddit', ''))
            top_comments = get_comments(post_id)
            time.sleep(0.5)
            if top_comments:
                print(f"   ğŸ’¬ Reddit ë°˜ì‘ í™•ë³´ (r/{subreddit_name}, overlap={best_score}): ëŒ“ê¸€ {len(top_comments)}ê°œ")
                return "\n".join([f"- {c}" for c in top_comments])
            else:
                print(f"   âš ï¸ Reddit ê²Œì‹œë¬¼ ë°œê²¬í–ˆì§€ë§Œ ëŒ“ê¸€ ì—†ìŒ")
        else:
            print(f"   â„¹ï¸ Reddit ê´€ë ¨ ê²Œì‹œë¬¼ ì—†ìŒ (best overlap={best_score})")

    except Exception as e:
        print(f"   âš ï¸ Reddit íŒŒì‹± ì‹¤íŒ¨: {e}")

    return ""

def fetch_wikipedia_image(keywords):
    """AI ì¶”ì¶œ í‚¤ì›Œë“œë¥¼ Wikipedia APIë¡œ ê²€ìƒ‰í•˜ì—¬ ì´ë¯¸ì§€ URL ë°˜í™˜ (CC ë¼ì´ì„ ìŠ¤)"""
    for keyword in keywords[:4]:  # ìµœëŒ€ 4ê°œ í‚¤ì›Œë“œ ìˆœì„œëŒ€ë¡œ ì‹œë„
        try:
            url = (
                "https://en.wikipedia.org/w/api.php"
                f"?action=query&titles={requests.utils.quote(str(keyword))}"
                "&prop=pageimages&format=json&pithumbsize=800"
            )
            resp = requests.get(url, timeout=5,
                                headers={"User-Agent": "CollectiveMonologue/1.0"})
            data = resp.json()
            pages = data.get("query", {}).get("pages", {})
            for page in pages.values():
                thumb = page.get("thumbnail", {}).get("source", "")
                if thumb:
                    print(f"   ğŸ–¼ï¸ Wikipedia ì´ë¯¸ì§€ í™•ë³´: [{keyword}]")
                    return thumb
        except Exception as e:
            print(f"   âš ï¸ Wikipedia ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
    return ""

def translate_and_summarize(text, title, reddit_comments=""):
    if not GEMINI_API_KEY:
        return {"title_en": title, "summary_en": "No API Key provided.", "keywords": []}

    if not text or len(text) < 50:
        return {"title_en": title, "summary_en": "Content too short or extraction failed.", "keywords": []}
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë³´ëƒ„ (í† í° ì œí•œ ë°©ì§€)
    truncated_text = text[:4000]

    reddit_section = ""
    if reddit_comments:
        reddit_section = f"""
    Additional Context (Community Comments - Local Fan Reactions):
    {reddit_comments}
    
    IMPORTANT: You have local fan reactions from a community. Synthesize these authentic reactions into your editorial. Describe what the US fans are excited about, worried about, or debating regarding this news. Do NOT mention specific names like 'Reddit' or 'Reddit community'. Just refer to them as 'í˜„ì§€ ì»¤ë®¤ë‹ˆí‹°' or 'í˜„ì§€ ë°˜ì‘'.
        """

    prompt = f"""
    You are the Chief Editor of "Collective Monologue", a premium Korean-language magazine dedicated to covering American theater with unparalleled depth, nuance, and cultural context.
    
    Below is an article titled '{title}'. Your task is to produce a high-quality, rich HTML-formatted Korean editorial that incorporates the following 4 key elements:

    1. **Magazine-Style Structuring**: Use appropriate HTML tags within the content. Use `<h3>` for logical subheadings, `<ul>` and `<li>` for key bullet points, and `<blockquote>` for pulling out powerful quotes or core messages to make the text visually engaging. Center-align appropriate texts.
    2. **Editor's Note (ì—ë””í„°ì˜ ì‹œì„ )**: At the end of the main news content, include a section titled `<h3>[ì—ë””í„°ì˜ ì‹œì„ ]</h3>` followed by your insightful analysis on what this news means for Korean readers, the industry context, or its broader cultural impact.
    3. **Positive & Negative Fandom Analysis (í˜„ì§€ íŒ¬ë“¤ì˜ ì‹œì„ : POSITIVE & NEGATIVE)**: Analyze the local community reactions to present a balanced view. Create an `<h3>[í˜„ì§€ íŒ¬ë“¤ì˜ ì‹œì„ : POSITIVE & NEGATIVE]</h3>` section detailing what fans are excited about (POSITIVE) and what they are worried about or debating (NEGATIVE). Do NOT reveal that the source is Reddit. Use general terms like "í˜„ì§€ ì»¤ë®¤ë‹ˆí‹°ì—ì„œëŠ”...".
    4. **Keyword Dictionary (ìš©ì–´ í•œ ìŠ¤í‘¼)**: Select 1 or 2 specialized terms related to American theater mentioned in the article, and create an `<h3>[ìš©ì–´ í•œ ìŠ¤í‘¼]</h3>` section. Explain them deeply to beginners (e.g., explaining "Off-Broadway", "Limited Run", "Swing", etc.) as if you are the 'ì œë¯¸ë‚˜ì´í”Œë˜ì‹œ í¸ì§‘ì(Gemini Flash Editor)' summarizing multiple sources cleanly. Do NOT mention specific sources like 'Wikipedia' or 'Reddit'. Avoid making it controversial.

    Write as a highly knowledgeable, warm, and insightful Korean cultural journalist from the 'Collective Monologue' editorial board.
    The output MUST be a valid JSON object with the following structure. Pay special attention to escaping HTML quotes properly (use single quotes inside the HTML string to avoid invalidating JSON, e.g. `<div class='example'>`), but do not break the JSON format:
    {{
        "title_kr": "ê¸°ì‚¬ì˜ ë³¸ì§ˆì„ ê¿°ëš«ëŠ” ë§¤ë ¥ì ì¸ ì œëª© (í•œêµ­ì–´)",
        "summary_kr": "ë©”ì¸ í˜ì´ì§€ì— í‘œì‹œë  1-2ë¬¸ì¥ì˜ í•µì‹¬ ìš”ì•½ (í•œêµ­ì–´)",
        "content_kr": "ì™„ì „í•œ HTML í˜•íƒœì˜ ê¸°ì‚¬ ë³¸ë¬¸. ë‰´ìŠ¤ ì½”ì–´ ë‚´ìš© -> [ì—ë””í„°ì˜ ì‹œì„ ] -> [í˜„ì§€ íŒ¬ë“¤ì˜ ì‹œì„ : POSITIVE & NEGATIVE] -> [ìš©ì–´ í•œ ìŠ¤í‘¼] ìˆœì„œë¡œ í’ë¶€í•˜ê²Œ êµ¬ì„±. `<p>`, `<h3>`, `<blockquote>`, `<ul>`, `<li>` ë“± íƒœê·¸ ì ê·¹ í™œìš©.",
        "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
    }}

    Article Body:
    {truncated_text}
    """

    # ì¬ì‹œë„ ë¡œì§ (Exponential Backoff)
    max_retries = 3
    base_delay = 5  # 5ì´ˆë¶€í„° ì‹œì‘

    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
            return json.loads(response.text)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                wait_time = base_delay * (2 ** attempt) + (attempt * 2) # Jitter ì¶”ê°€
                print(f"âš ï¸ Quota exceeded. Retrying in {wait_time}s... (Attempt {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ Summary failed (API Error): {e}")
                break
    
    return {
        "title_en": title,
        "summary_en": "Summarization failed.",
        "title_kr": title,
        "summary_kr": "ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        "content_kr": "ë³¸ë¬¸ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
        "keywords": []
    }

def fetch_broadway_grosses():
    """Playbill.comì—ì„œ ì´ë²ˆ ì£¼ ë¸Œë¡œë“œì›¨ì´ ë°•ìŠ¤ì˜¤í”¼ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = "https://www.playbill.com/grosses"
    headers = {"User-Agent": REDDIT_USER_AGENT}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = bs4.BeautifulSoup(resp.content, 'html.parser')
            table = soup.find('table')
            if not table:
                return []
            
            rows = table.find('tbody').find_all('tr') if table.find('tbody') else table.find_all('tr')[1:]
            grosses = []
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 8:
                    # ê³µì—°ëª…ê³¼ ê·¹ì¥ëª… ë¶„ë¦¬
                    full_text = cols[0].get_text(strip=True)
                    show_node = cols[0].find('a')
                    show_name = show_node.get_text(strip=True) if show_node else full_text
                    # ê·¹ì¥ëª…: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê³µì—°ëª…ì„ ë¹¼ë©´ ê·¹ì¥ëª…ë§Œ ë‚¨ìŒ
                    theater_name = full_text.replace(show_name, '').strip()
                    
                    gross_str = cols[1].get_text(strip=True)
                    diff_str = cols[2].get_text(strip=True)  # ì „ì£¼ ëŒ€ë¹„ ë³€ë™
                    avg_ticket_str = cols[3].get_text(strip=True)  # í‰ê·  í‹°ì¼“ ê°€ê²©
                    attendance_str = cols[4].get_text(strip=True)  # ê´€ê° ìˆ˜
                    capacity_str = cols[6].get_text(strip=True)
                    
                    try:
                        parsed_gross = float(gross_str.replace('$', '').replace(',', ''))
                        grosses.append({
                            "show": show_name,
                            "theater": theater_name,
                            "gross_formatted": gross_str,
                            "gross": parsed_gross,
                            "diff": diff_str,
                            "avg_ticket": avg_ticket_str,
                            "attendance": attendance_str,
                            "capacity": capacity_str
                        })
                    except ValueError:
                        continue
                        
            # ë§¤ì¶œì•¡(Gross) ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ ì¶”ì¶œ
            grosses.sort(key=lambda x: x['gross'], reverse=True)
            top5 = grosses[:5]
            for i, item in enumerate(top5):
                item['rank'] = i + 1
            
            # LLMìœ¼ë¡œ ê° ê³µì—°ì— ëŒ€í•œ í•œ ì¤„ í•œêµ­ì–´ ì†Œê°œ ì¶”ê°€
            if GEMINI_API_KEY and top5:
                try:
                    show_list = ", ".join([s['show'] for s in top5])
                    desc_prompt = f"""ì•„ë˜ 5ê°œ ë¸Œë¡œë“œì›¨ì´ ê³µì—°ì— ëŒ€í•´ ê°ê° í•œêµ­ì–´ë¡œ í•œ ì¤„(15~25ì) ì†Œê°œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë®¤ì§€ì»¬ì´ë©´ ì¥ë¥´ì™€ ë¶„ìœ„ê¸°ë¥¼, ì—°ê·¹ì´ë©´ ì£¼ì œë‚˜ ë°°ìš°ë¥¼ ê°„ë‹¨íˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ JSON ê°ì²´ í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. í‚¤ëŠ” ê³µì—°ëª…(ì˜ë¬¸), ê°’ì€ í•œêµ­ì–´ ì†Œê°œì…ë‹ˆë‹¤.

ê³µì—° ëª©ë¡: {show_list}

ì˜ˆì‹œ: {{"Hamilton": "ë¯¸êµ­ ê±´êµ­ì˜ ì—­ì‚¬ë¥¼ í™í•©ìœ¼ë¡œ í’€ì–´ë‚¸ ë®¤ì§€ì»¬"}}
"""
                    desc_resp = model.generate_content(desc_prompt).text.strip()
                    if desc_resp.startswith("```json"):
                        desc_resp = desc_resp[7:]
                    if desc_resp.endswith("```"):
                        desc_resp = desc_resp[:-3]
                    descriptions = json.loads(desc_resp.strip())
                    for item in top5:
                        item['description_kr'] = descriptions.get(item['show'], '')
                except Exception as e:
                    print(f"   âš ï¸ ê³µì—° ì†Œê°œ ìƒì„± ìŠ¤í‚µ: {e}")
            
            return top5
    except Exception as e:
        print(f"   âš ï¸ ë¸Œë¡œë“œì›¨ì´ ë°•ìŠ¤ì˜¤í”¼ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
    return []

def generate_weekly_recommendations(articles_data):
    """ìµœê·¼ ì¸ë”” ë§¤ì²´ ì¤‘ì‹¬ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´/ì‹œì¹´ê³  ì¶”ì²œì‘ 3ê°œë¥¼ ë½‘ìŠµë‹ˆë‹¤."""
    if not GEMINI_API_KEY or not articles_data:
        return []

    context = ""
    for idx, article in enumerate(articles_data[:10]):
        context += f"[{idx+1}] ì œëª©: {article['title_kr']}\nìš”ì•½: {article['summary_kr']}\n\n"
        
    prompt = f"""
    ë‹¹ì‹ ì€ ë¯¸êµ­ ì—°ê·¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì´ë²ˆ ì£¼ ìˆ˜ì§‘ëœ ì—°ê·¹ ê¸°ì‚¬ë“¤ì˜ ëª©ë¡ì…ë‹ˆë‹¤.
    ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´ ë˜ëŠ” ì‹œì¹´ê³  ë“± ì§€ì—­ ì—°ê·¹/í™”ì œì‘ ì¤‘ ì¶”ì²œí•  ë§Œí•œ ì‘í’ˆ 3ê°œ**ë¥¼ ì„ ì •í•´ ì§§ê²Œ ì¶”ì²œ ì´ìœ ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ê¸°ì‚¬ ëª©ë¡:
    {context}

    ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”. ê° ê°ì²´ëŠ” "title" (ì‘í’ˆëª…, í•œê¸€/ì˜ë¬¸ ë³‘ê¸°), "reason" (ì¶”ì²œ ì´ìœ , 2ë¬¸ì¥ ì´ë‚´) í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
    ê¸°ì‚¬ ë‚´ìš© ì¤‘ ì¶”ì²œí•  ë§Œí•œ êµ¬ì²´ì ì¸ ì—°ê·¹ ì‘í’ˆì´ ë¶€ì¡±í•˜ë‹¤ë©´, í˜„ì¬ ë¯¸êµ­ì—ì„œ í‰ë‹¨ì˜ ë†’ì€ ì§€ì§€ë¥¼ ë°›ê³  ìˆëŠ” ì˜¤í”„-ë¸Œë¡œë“œì›¨ì´ í™”ì œì‘ì„ ì„ì˜ë¡œ ê³¨ë¼ë„ ì¢‹ìŠµë‹ˆë‹¤.
    
    [
        {{"title": "ë¯¼ì¤‘ì˜ ì  (An Enemy of the People)", "reason": "ì œë ˆë¯¸ ìŠ¤íŠ¸ë¡±ì˜ ëª…ì—°ê¸°ì™€ í•¨ê»˜ í™˜ê²½ ë¬¸ì œë¼ëŠ” ì‹œëŒ€ì  í™”ë‘ë¥¼ ë˜ì§€ëŠ” í•„ëŒ ì—°ê·¹ì…ë‹ˆë‹¤."}}
    ]
    ë‹µë³€ì€ ì˜¤ì§ JSON í˜•ì‹ìœ¼ë¡œë§Œ í•´ì£¼ì„¸ìš”.
    """
    try:
        response = model.generate_content(prompt).text.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.endswith("```"):
            response = response[:-3]
        return json.loads(response.strip())
    except Exception as e:
        print(f"   âš ï¸ ì£¼ê°„ ì¶”ì²œì‘ ìƒì„± ì‹¤íŒ¨: {e}")
    return []

def process_entry(entry, source, tier):
    """Process individual article (for parallel execution)"""
    title = entry.title
    link = entry.link
    published = entry.get('published', datetime.now().strftime("%Y-%m-%d"))
    
    print(f"   Analyzing [{tier.upper()}]: {title[:30]}...")

    # 1. Extract full text + image from article HTML
    full_text, html_image = fetch_article_content(link)
    
    # 2. Extract local reactions from Reddit (URL & Title based)
    reddit_comments = fetch_reddit_comments(link, title, [])
    
    # 3. AI Summary with Reddit context
    ai_result = translate_and_summarize(full_text, title, reddit_comments)

    # Extract image: RSS metadata ìš°ì„ , ì—†ìœ¼ë©´ HTML íŒŒì‹± (og:image)
    image_url = ""
    if 'media_content' in entry and len(entry.media_content) > 0:
        image_url = entry.media_content[0].get('url', '')
    elif 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
        image_url = entry.media_thumbnail[0].get('url', '')
    elif 'enclosures' in entry and len(entry.enclosures) > 0:
        for enc in entry.enclosures:
            if enc.get('type', '').startswith('image/'):
                image_url = enc.get('href', '')
                break

    if not image_url:
        image_url = html_image
        
    # ìµœì¢… ì •í¬ ì´ë¯¸ì§€ ë°©ì–´ì„  (ì¶”ì  í”½ì…€ í•„í„°ë§)
    if image_url and any(x in image_url.lower() for x in ['scorecardresearch', 'pixel', '1x1', 'avatar', 'icon', 'blank']):
        image_url = ""

    # RSSë‚˜ HTMLì—ì„œ ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ìœ¼ë©´ Wikipedia ì´ë¯¸ì§€ ê²€ìƒ‰ (í•­ìƒ ì‹œë„)
    if not image_url:
        import re
        original_title_words = entry.title
        en_keywords = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+', original_title_words)
        search_keywords = en_keywords + [str(k) for k in ai_result.get('keywords', [])]
        if search_keywords:
            image_url = fetch_wikipedia_image(search_keywords)

    # Validate the data before returning
    summary_kr = ai_result.get('summary_kr', 'ë‚´ìš© ì—†ìŒ').strip()
    if not summary_kr or "ë‚´ìš© ì—†ìŒ" in summary_kr or len(summary_kr) < 20:
        print(f"   âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ê¸°ì‚¬ ë‚´ìš© ì œì™¸: {title[:30]}")
        return None
        
    # ì—„ê²©í•œ ì´ë¯¸ì§€ ë³´ì¥: Wikipedia í´ë°±ê¹Œì§€ ì‹¤íŒ¨í•˜ë©´ ê¸°ì‚¬ë¥¼ ë²„ë¦½ë‹ˆë‹¤ (ì„ íƒì )
    # í˜„ì¬ ì •ì±…: ì´ë¯¸ì§€ê°€ ì—†ë”ë¼ë„ ë³¸ë¬¸ì´ í›Œë¥­í•˜ë©´ ì‚´ë¦½ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼
    # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë²„ë¦¬ëŠ” ì—„ê²©í•œ ì •ì±…ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    # ì¼ë‹¨ì€ ì´ë¯¸ì§€ê°€ ì—†ì„ ê²½ìš° í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ëƒ…ë‘ë˜, ë¹ˆ ë‚´ìš©ì„ ë§‰ëŠ” ë° ì§‘ì¤‘í•©ë‹ˆë‹¤.
    # ì‚¬ìš©ìê°€ "ì‚¬ì§„ë„ ìˆì§€ ì•ŠìŒ"ì´ë¼ê³  í–ˆìœ¼ë¯€ë¡œ ì‚¬ì§„ì´ Noneì´ë©´ ë²„ë¦¬ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.
    if not image_url:
         print(f"   âš ï¸ ì´ë¯¸ì§€ í™•ë³´ ì‹¤íŒ¨ ê¸°ì‚¬ ì œì™¸: {title[:30]}")
         return None

    return {
        "source": source,
        "tier": tier,  # 'major' ë˜ëŠ” 'indie'
        "original_title": title,
        "link": link,
        "image": image_url,
        "title_kr": ai_result.get('title_kr', title),
        "summary_kr": ai_result.get('summary_kr', 'ë‚´ìš© ì—†ìŒ'),
        "content_kr": ai_result.get('content_kr', 'ë‚´ìš© ì—†ìŒ'),
        "reddit_reaction_kr": ai_result.get('reddit_reaction_kr', ''),
        "keywords": ai_result.get('keywords', []),
        "date": published,
        "scraped_at": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S") # Changed to timezone.utc
    }

def send_email(articles):
    if not EMAIL_SENDER or not EMAIL_PASSWORD or not articles:
        return

    subject = f"[StageSide] Latest News Briefing - {datetime.now().strftime('%Y-%m-%d %H:%M')}" # Kept datetime.now() as per instruction for subject
    body = "<h2>Today's Top News</h2><br>"
    
    for article in articles:
        body += f"<h3>[{article['source']}] {article['title_kr']}</h3>"
        body += f"<p>{article['summary_kr']}</p>"
        body += f"<p><small>Keywords: {', '.join(article['keywords'])}</small></p>"
        body += f"<p><a href='{article['link']}' target='_blank'>Read Original</a></p><hr>"

    msg = MIMEText(body, 'html')
    msg['Subject'] = subject
    msg['From'] = EMAIL_SENDER
    msg['To'] = EMAIL_RECEIVER

    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:
            server.login(EMAIL_SENDER, EMAIL_PASSWORD)
            server.send_message(msg)
        print(f"ğŸ“§ ì´ë©”ì¼ ë ˆí¬íŠ¸ ë°œì†¡ ì™„ë£Œ ({len(articles)}ê±´)")
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")

def save_to_json(major_articles, indie_articles):
    file_path = 'data/articles.json'
    os.makedirs("data", exist_ok=True)
    
    new_data = major_articles + indie_articles
    
    existing_data = []
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except Exception as e:
            print(f"ê¸°ì¡´ ê¸°ì‚¬ ë¡œë“œ ì˜¤ë¥˜: {e}")
            
    # ëˆ„ì : ìš°ì„  ê¸°ì¡´ ë°ì´í„°ë¥¼ ë„£ê³ , ìƒˆë¡œìš´ ë°ì´í„°ì™€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ë§í¬ ê¸°ì¤€ ë¨¸ì§€
    existing_links = {item['link']: item for item in existing_data}
    
    actually_new = []
    for item in new_data:
        if item['link'] not in existing_links:
            existing_data.insert(0, item) # ë§¨ ì•ì— ì¶”ê°€ (ìµœì‹ ìˆœ)
            existing_links[item['link']] = item
            actually_new.append(item)
            
    # ì „ì²´ ê°¯ìˆ˜ ì œí•œ (ì˜ˆ: 60ê°œ ìœ ì§€í•˜ì—¬ ì‚¬ì´íŠ¸ í’ˆì§ˆ ìœ ì§€ ë° ê¸°ì‚¬ ëˆ„ì )
    final_data = existing_data[:60]

    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=4)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: ì‹ ê·œ ì¶”ê°€ {len(actually_new)}ê±´, ì´ ìœ ì§€ {len(final_data)}ê±´")
    
    # Sitemap ìƒì„± (ì „ì²´ ë°ì´í„° ê¸°ë°˜)
    generate_sitemap(final_data)
    return actually_new

def generate_sitemap(articles):
    """
    ì €ì¥ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ê¸€ ê²€ìƒ‰ìš© sitemap.xmlì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    import urllib.parse
    base_url = "https://collectivemonologue.pages.dev"
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    xml_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
    xml_content += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    # ê¸°ë³¸ ì •ì  í˜ì´ì§€ë“¤
    static_pages = ['index.html', 'category.html', 'about.html', 'contact.html', 'privacy.html']
    for page in static_pages:
        xml_content += '  <url>\n'
        xml_content += f'    <loc>{base_url}/{page}</loc>\n'
        xml_content += f'    <lastmod>{today}</lastmod>\n'
        xml_content += '    <changefreq>daily</changefreq>\n'
        xml_content += '    <priority>0.8</priority>\n'
        xml_content += '  </url>\n'

    # ë™ì  ê¸°ì‚¬ í˜ì´ì§€ë“¤ (article.html?id=...)
    for article in articles:
        # ì˜ë¬¸ íƒ€ì´í‹€ì„ ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ íŒŒì‹±í•˜ì—¬ ID ìƒì„± (frontend main.jsì™€ ë™ì¼ ë¡œì§ ì˜ˆìƒ)
        safe_title = "".join(c if c.isalnum() or c.isspace() else "" for c in article['original_title']).strip()
        article_id = safe_title.replace(' ', '_').lower()
        encoded_id = urllib.parse.quote(article_id)
        
        xml_content += '  <url>\n'
        xml_content += f'    <loc>{base_url}/article.html?id={encoded_id}</loc>\n'
        xml_content += f'    <lastmod>{today}</lastmod>\n'
        xml_content += '    <changefreq>weekly</changefreq>\n'
        xml_content += '    <priority>0.6</priority>\n'
        xml_content += '  </url>\n'

    xml_content += '</urlset>'

    with open('sitemap.xml', 'w', encoding='utf-8') as f:
        f.write(xml_content)
    print("âœ… sitemap.xml ìë™ ìƒì„± ì™„ë£Œ")

def crawl_rss():
    print("ğŸš€ í¬ë¡¤ëŸ¬(ver.2) ì‹œì‘ â€” ë©”ì´ì € 5ê±´ + ì¸ë”” 4ê±´ ìˆ˜ì§‘")
    
    def fetch_from_feeds(feeds_dict, tier):
        entries = []
        for source, url in feeds_dict.items():
            print(f"ğŸ“¡ [{tier.upper()}] {source} ê²€ìƒ‰ ì¤‘...")
            try:
                feed = feedparser.parse(url)
                # ê° ì†ŒìŠ¤ë³„ ìµœì‹  2ê°œì”© ìˆ˜ì§‘ (ë²„í¼ í™•ë³´: 1ê°œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ê²ƒìœ¼ë¡œ ëŒ€ì²´)
                for entry in feed.entries[:2]:
                    entries.append((entry, source, tier))
            except Exception as e:
                print(f"âš ï¸ {source} í”¼ë“œ ì˜¤ë¥˜: {e}")
        return entries

    major_entries = fetch_from_feeds(MAJOR_FEEDS, 'major')
    indie_entries = fetch_from_feeds(INDIE_FEEDS, 'indie')
    all_entries = major_entries + indie_entries
    
    print(f"ì´ {len(all_entries)}ê°œ ê¸°ì‚¬ ë°œê²¬. ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

    major_results = []
    indie_results = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_to_entry = {
            executor.submit(process_entry, entry, source, tier): (entry, source, tier)
            for entry, source, tier in all_entries
        }
        for future in concurrent.futures.as_completed(future_to_entry):
            try:
                data = future.result()
                if data: # ê²€ì¦ í†µê³¼í•œ ë°ì´í„°ë§Œ ì¶”ê°€
                    if data['tier'] == 'major':
                        major_results.append(data)
                    else:
                        indie_results.append(data)
            except Exception as exc:
                print(f"âŒ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {exc}")

    return major_results, indie_results

if __name__ == "__main__":
    major_data, indie_data = crawl_rss()
    if major_data or indie_data:
        new_added = save_to_json(major_data, indie_data)
        if new_added:
            send_email(new_added)
        else:
            print("ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ì‚¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì´ë©”ì¼ì„ ë°œì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
